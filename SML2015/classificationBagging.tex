\documentclass[12pt]{beamer}
%\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{_defsAndPackages675notation}
\usepackage{_defsAndPackages675beamer}

%\DeclareMathSizes{12}{12}{5}{12}
\newcommand{\parenthetical}[2]{#1  \scriptstyle \alr{( #2)}}
\begin{document}

\title{\alg{Randomization methods: Bagging}}
\subtitle{\classTitle}
%\author{\alg{Darren Homrighausen, PhD}}
%\institute{\classTitle}
\date{}



\begin{frame}
\maketitle

\organization
%
\end{frame}

\begin{frame}[fragile]
\frametitle{Notation}
\smallCapGreen{Reminder:} For either \alo{classification} or \alo{regression}, we produce \alg{predictions}
for a given covariate vector $X$

\vsp
That is, we form
\[
\hat{Y} =  \hat{f}(X) \qquad \textrm{or} \qquad \hat{Y} =  \hat{g}(X)
\]
where
\begin{itemize}
\item $\hat{f}$ or $\hat{g}$ is some procedure formed with the \alo{training data}

\script{\smallCapGreen{Examples:} $\hat\beta$ formed by least squares}
\item The prediction $\hat{Y}$ formed at a desired covariate vector $X$

\script{\smallCapGreen{Example:} $\hat{Y} = X^{\top}\hat\beta$ formed by least squares}

\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Bagging}
Many methods (trees included) tend to be designed to have lower bias but high variance

\vsp
This means
that if we split the training data into two parts at random and fit a decision tree to each part, the results could be quite \alo{different}

\vsp
A low variance estimator would yield \alo{similar} results if applied repeatedly to distinct data sets 

\script{consider $\hat f(X) = 0$ for all $X$}

\vsp
\alg{Bagging}, also known as \alg{Bootstrap AGgregation}, is a general purpose procedure for reducing variance. 

\vsp
We'll use it specifically in the context of trees, but it can be applied more broadly.
\end{frame}

\begin{frame}[fragile]
\frametitle{Bagging: The main idea}
Suppose we have $n$ uncorrelated observations $Z_1, \ldots, Z_n$, each with variance $\sigma^2$.

\vsp
What is the variance of
\[
\overline{Z} = \frac{1}{n} \sum_{i=1}^n Z_i?
\]
\pause
\smallCapGreen{Answer:} $\sigma^2/n$.

\vsp
More generally, if we have $B$ separate (uncorrelated) training sets, we could form $B$ separate model fits, 
\[
\hat f^1(X), \ldots, \hat f^B(X)
\]
Then average them:
\[
\hat f_{B}(X) = \frac{1}{B} \sum_{b=1}^B \hat f^b(X)
\]
\end{frame}

\begin{frame}[fragile]
\frametitle{Bagging: The bootstrap part}
Of course, this isn't practical as having access to many training sets is unlikely.  

\vsp

We therefore
turn to the \alg{bootstrap} to simulate having many training sets.

\vsp
The bootstrap is a widely applicable statistical tool that can be used to quantify uncertainty without Gaussian approximations.

\vsp
Let's look at an example.
\end{frame}

\begin{frame}
      \begin{center}
        \alg{\huge Bootstrap detour}
      \end{center}
\end{frame}

\begin{frame}[fragile]
\frametitle{Bootstrap detour}
Suppose we are looking to invest in two financial instruments, $X$ and $Y$.
The return on these investments is random, but we still want to allocate our money in a risk minimizing way.

\vsp
That is, for some $\alpha \in (0,1)$, we want to minimize
\[
\textrm{Var}(\alpha X + (1-\alpha)Y) 
\]
The minimizing $\alpha$ is:
\[
\alpha_* = \frac{\sigma_Y^2 - \sigma_{XY}^2}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}^2}
\]
\script{Here, $\sigma_{XY}^2$ is the \alg{covariance} between $X$ and $Y$}

\vsp
which we can estimate via
\[
\hat\alpha = \frac{\hat\sigma_Y^2 - \hat\sigma_{XY}^2}{\hat\sigma_X^2 + \hat\sigma_Y^2 - 2\hat\sigma_{XY}^2}
\]
\end{frame}


\begin{frame}[fragile]
\frametitle{Bootstrap detour}
Now that we have an estimator of $\alpha$, it would be nice to have an estimator of its \alo{variability}.  In this
case, computing a standard error is difficult. 
\vsp

Suppose for a moment that we can simulate a large number of draws (say 1000) of the data, which has actual
value $\alpha = 0.6$. 

Then we could
get estimates $\hat\alpha_1 ,\ldots, \hat \alpha_{1000}$: 
\begin{figure}
\centering
\includegraphics[width=2in,trim=0 40 0 40,clip]{../figures/recessionTreesBootstrap1.pdf}
\caption*{This is the \alg{sampling distribution} of $\hat\alpha$}
\end{figure}
\end{frame}

\begin{frame}[fragile]
\frametitle{Bootstrap detour}
\begin{figure}
\centering
\includegraphics[width=1.8in,trim=0 40 0 40,clip]{../figures/recessionTreesBootstrap1.pdf}
\end{figure}

The mean of all of these is:
\[
\overline{\alpha} = \frac{1}{1000} \sum_{r=1}^{1000} \hat\alpha_r = 0.599,
\]
which is very close to 0.6 (red line), 
and the standard error is
\[
\sqrt{ \frac{1}{1000 -1} \sum_{r=1}^{1000} (\hat\alpha_r - \overline{\alpha})^2 }= 0.035.
\]
\end{frame}

\begin{frame}[fragile]
\frametitle{Bootstrap detour}
The standard error of  0.035 gives a very good idea of the accuracy of $\hat \alpha$ for a single sample.  Roughly
speaking, for a new random sample, we expect $\hat\alpha \in (\alpha - 2*0.035, \alpha + 2*0.035)$.

\vsp
In practice, of course, we cannot use this procedure as it relies on being able to draw a large number of 
(independent) samples from the same distribution as our data.  

\vsp
This is where the \alg{bootstrap} comes in.

\vsp
We instead draw a large number of samples directly from our observed data.  This sampling is done
\alo{with replacement}, which means that the same data point can be drawn multiple times.
\end{frame}

\begin{frame}[fragile]
\frametitle{Bootstrap detour: Small example}
Suppose we have data $\data = (4.3,3,7.2,6.9,5.5)$.

\vsp
Then we can draw bootstrap samples, which might look like:
\begin{align*}
\data_1^* &= (7.2,4.3,7.2,5.5,6.9) \\
\data_2^* &= (6.9, 4.3 ,3.0 ,4.3 ,6.9) \\
&\ \ \vdots \\
\data_B^* &= (4.3 ,3.0 ,3.0 ,5.5 ,6.9)
\end{align*}
It turns out each of these $\data_b^*$ have \alo{very} similar properties as $\data$
\end{frame}

\begin{frame}[fragile]
\frametitle{Bootstrap detour: Small example}
Now, we form the bootstrap mean:
\[
\textrm{mean}_B = \frac{1}{B} \sum_{b=1}^B \hat\alpha_b^{*} 
\]
\vsp

The bootstrap estimator of the standard error is:
\[
\textrm{SE}_B = \sqrt{\frac{1}{B} \sum_{b=1}^B \left(\hat\alpha_b^{*} - \textrm{mean}_B\right)^2}
\]

\end{frame}

\begin{frame}[fragile]
\frametitle{Bootstrap detour}
%Now, the bootstrap estimator of the standard error is:
%\[
%\textrm{SE}_B(\hat\alpha) = \sqrt{\frac{1}{B-1} \sum_{b=1}^B \left(\hat\alpha_b^{*} - \frac{1}{B}\sum_{s=1}^B \hat\alpha_s^{*}\right)^2}
%\]

\begin{table}
\begin{tabular}{p{2in}p{2.5in}}
\includegraphics[width=2.1in,trim=0 40 0 40,clip]{../figures/recessionTreesBootstrap1.pdf} &
\includegraphics[width=2.1in,trim=0 40 0 40,clip]{../figures/recessionTreesBootstrap2.pdf} \\
Sampling distribution of $\hat\alpha$   & Bootstrap distribution of $\hat\alpha$ \\
(impossible to form)  & (possible to form)  \\
\end{tabular}
\end{table}
\end{frame}

\begin{frame}
\frametitle{Bootstrap: End detour}
\smallCapGreen{Summary:}
\vsp

Suppose we have data $\data = (Z_1, \ldots, Z_n)$ and we want to get an idea of the sampling
distribution of some statistic $\hat{f}$ trained on $\data$.  

\vsp
Then we do the following: Fix a large number $B$

\script{$B$ could be, say, 1000}

\vsp 
Then for each $b = 1,\ldots,B$
\begin{enumerate}
\item Form a new bootstrap draw from $\data$, call it $\data^*$
\item Compute $\hat f_b^*$ from $\data^*$ 
\end{enumerate}
Now, we can estimate the distribution of $\hat f$ trained on $\data$ by looking at the 
\alo{distribution} of the $B$ draws, $\hat f_b^*$
\end{frame}

\begin{frame}
             \begin{center} 
  \alg{\huge End detour}
        \end{center}
\end{frame}

\begin{frame}[fragile]
\frametitle{Bagging: The bootstrap part}
Now, instead of having $B$ separate training sets, we train on  $B$ bootstrap draws:
\[
\hat f_1^{*}(X), \ldots, \hat f_B^{*}(X)
\]
and then average them:
\[
\hat f_{\textrm{bag}}(X) = \frac{1}{B} \sum_{b=1}^B \hat f_b^{*}(X)
\]
This process is known as \alg{Bagging}
\end{frame}

\begin{frame}
  \begin{columns}[c]
    \begin{column}{.45\paperwidth}
      \begin{center}
        \alg{\huge Bagging trees}
      \end{center}
    \end{column}
    \begin{column}{.5\paperwidth}
      \begin{center}
        \includegraphics[height=.8\paperheight]{../figures/bagtree.jpg}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}[fragile]
\frametitle{Bagging trees}
The procedure for trees is the following
\begin{enumerate}
\item Choose a large number $B$.
\item For each $b = 1,\ldots, B$, grow an unpruned tree on the $b^{th}$ bootstrap draw from the data.
\item Average all these trees together.
\end{enumerate}
\vsp

Each tree, since it is unpruned, will have (low/\textcolor<2->{redmain}{high}) variance and
(\textcolor<2->{redmain}{low}/high) bias

\vsp
\pause
Therefore averaging many trees results in an estimator that has lower variance and still low bias.
\end{frame}

\transitionSlide{Additional tree bagging topics}


\begin{frame}[fragile]
\frametitle{Bagging trees}
Now that we are growing a large number ($B$) of random trees, we can't directly look at 
the \alo{dendrogram}

\vsp
We no longer have that nice diagram that shows the segmentation of the predictor space 
\script{More accurately, we have $B$ of them}

\vsp
However, we do get some helpful information instead:

\vsp

\begin{itemize}
\item Mean decrease variable importance
\item Permutation variable importance
\item Out-of-Bag error estimation (OOB)

\script{Each time a tree is grown,
we can get its prediction error on the unused observations.  We average this over all bootstrap samples}
\item Proximity plot
\end{itemize}
\end{frame}



\begin{frame}[fragile]
\frametitle{Mean decrease variable importance}

\smallCapGreen{Observation:} At every split of a node,
the loss function decreases

\vvsp
Hence, adding up the \alo{amount of decrease} for each covariate over all trees
gives an indication of feature importance

\vsp
Intuitively an important covariate is one that if split upon, it leads to a large drop in loss function
\end{frame}


\begin{frame}[fragile]
\frametitle{Mean decrease variable importance}
To recover some information, we can do the following:
\begin{itemize}
\item[1.] For each of the $B$ trees and each of the $p$ features, we record the amount that the Gini index 
(or cross-entropy) is reduced by the addition of that feature 
\item[2.] Report the average reduction over all $B$ trees
\end{itemize}

\vsp

This gives us an indication of the \alo{importance} of a feature
%\footnote{A very important caveat to this interpretation
%will be coming up when we talk about adding random, uncorrelated predictors}
\end{frame}

\begin{frame}[fragile]
\frametitle{Mean decrease variable importance}

\begin{figure}
\centering
\includegraphics[width=2.7in]{../figures/recessionTrees2baggingImportance}
%\includegraphics[width=2.3in]{../figures/recessionTreesRFimportance}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Out-of-Bag samples (OOB)}
One can show that, on average, drawing $n$ samples from $n$ observations with replacement  results
in about 2/3 of the observations being selected.

\vsp
The remaining one-third of the observations not used are referred to as \alg{out-of-bag (OOB)}
\end{frame}

\begin{frame}
\frametitle{Out-of-Bag samples (OOB)}


We can think of it as a for-free \alo{cross-validation}


\vsp
The observations that aren't included serve as \alg{test} data

\vsp
This provides a free estimate of prediction risk for each tree

\vsp
We can therefore get an overall estimate of prediction risk by averaging these estimates over \alo{all} bootstrapped trees
\end{frame}


\begin{frame}[fragile]
\frametitle{Permutation variable importance}
Consider the $b^{th}$ bootstrap sample

\begin{enumerate}
\item The OOB prediction accuracy is recorded for the $b^{th}$ tree
\item The $j^{th}$ feature is randomly permuted in the OOB sample

\script{ie: If $\data_{OOB,b} = (Z_{t_1},\ldots,Z_{t_{n/3}})$ then permute $X_{t}^j$ to form $\tilde{X}_{t}^j$
and hence $\tilde{Z}_{t_i} = (Y_{t_i},X_{t_i}^1,\ldots,\tilde{X}_{t_i}^j,\ldots,X_{t_i}^p)$.}
\item The prediction error is recomputed and the change in prediction error is recorded

\script{That is, get the OOB error rate for $\tilde{\data}_{OOB,b} = (\tilde{Z}_{t_1},\ldots,\tilde{Z}_{t_{n/3}})$}
\end{enumerate}
\vsp

\smallCapGreen{Intuition:} If a feature is highly important, then the OOB prediction error should increase substantially
after permuting the OOB values for that feature

\end{frame}


\begin{frame}
\frametitle{Proximity plot}
For the $b^{th}$ tree, we can examine which OOB observations are assigned to the same \alo{terminal node}

\vsp
Form an $n \times n$ matrix $P$ and increment $P[i,i'] \leftarrow P[i,i'] + 1$ if $Z_{i}$ and $Z_{i'}$ are assigned to the same terminal node

\vsp
Now, use some sort of dimension reduction technique to visualize the data in 2-3 dimensions

\script{Multidimensional scaling is most commonly used (between observation distances are preserved)}

\vsp
The idea is that even if the data have combinations of qualitative/quantitative variables and/or have high
dimension, we can view their similarity through the \alo{forward operator} of the bagged estimator
\end{frame}


\transitionSlide{Random forest}

%\begin{frame}[fragile]
%\frametitle{Bagging trees: Variable importance measures}
%Now that we are growing a large number ($B$) of random trees, we can't directly look at 
%the \alo{dendrogram}
%
%\vsp
%This means we have sacrificed some interpretability for better performance
%
%\vsp
%However, we do get some helpful information instead
%
%\vsp
%
%\begin{itemize}
%\item Out-of-Bag error estimation (OOB)
%%\item Variable importance
%\item Gini Importance
%\end{itemize}
%\end{frame}
%
%
%
%
%\begin{frame}
%\frametitle{Out-of-Bag error estimation (OOB)}
%One can show that, on average, drawing $n$ samples from $n$ observations with replacement  results
%in about 2/3 of the observations being selected.
%
%\vsp
%The remaining one-third of the observations not used are referred to as \alo{out-of-bag (OOB)}.
%
%\vsp
%We can think of it as a for-free cross-validation.  
%%In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:
%
%\vsp
%Each time a tree is grown,
%we can get its prediction error on the unused observations.  We average this over all bootstrap samples.
%\end{frame}
%

%\begin{frame}
%\frametitle{Variable importance}
%In every tree grown in the forest, put down the oob cases and count the number of votes cast for the correct class. Now randomly permute the values of variable m in the oob cases and put these cases down the tree. Subtract the number of votes for the correct class in the variable-m-permuted oob data from the number of votes for the correct class in the untouched oob data. The average of this number over all trees in the forest is the raw importance score for variable m.
%
%If the values of this score from tree to tree are independent, then the standard error can be computed by a standard computation. The correlations of these scores between trees have been computed for a number of data sets and proved to be quite low, therefore we compute standard errors in the classical way, divide the raw score by its standard error to get a z-score, ands assign a significance level to the z-score assuming normality.
%
%If the number of variables is very large, forests can be run once with all the variables, then run again using only the most important variables from the first run.
%
%For each case, consider all the trees for which it is oob. Subtract the percentage of votes for the correct class in the variable-m-permuted oob data from the percentage of votes for the correct class in the untouched oob data. This is the local importance score for variable m for this case, and is used in the graphics program RAFT.
%\end{frame}



\begin{frame}[fragile]
\frametitle{Random Forest}
\alg{Random Forest} is a small extension of Bagging, in which the bootstrap trees are \alo{decorrelated}  

\vsp
The idea is, we draw a bootstrap sample and start to build a tree. 
\begin{itemize}
\item[-] At each split, we randomly select
$m$ of the possible $p$ features as candidates for the split. 
\item[-] A new sample of size $m$ of the features is taken at each split. 
\end{itemize}   

\vsp
Usually, we use about $m = \sqrt{p}$ 

\script{this would be 7 out of 56 features for GDP data}

\vsp
In other words, at each split, we aren't even allowed to consider the majority of possible features!
\end{frame}

\begin{frame}[fragile]
\frametitle{Random Forest}
\alo{What is going on here?}

\vsp
Suppose there is 1 really strong feature and many mediocre ones. 

\begin{itemize}
\item[-] Then each tree will have this one feature in it,
\item[-] Therefore, each tree will look very \alo{similar} (i.e. highly correlated).  
\item[-] Averaging highly correlated things leads to much less variance reduction than if they were uncorrelated.
\end{itemize}
\vsp
If we don't allow some trees/splits to use this important feature, each of the trees will be much less similar and
hence much less correlated.

\vsp
Bagging is Random Forest when $m = p$, that is, when we can consider all the features at each split.
\end{frame}


\begin{frame}
\frametitle{ Random forest }
An average of $B$ i.i.d random variables has variance
\[
\frac{\sigma^2}{B}
\]

\vsp
An average of $B$ random variables has variance 
\[
\rho\sigma^2 + \frac{(1-\rho)\sigma^2}{B}
\]
for correlation $\rho$

\vsp
As $B \rightarrow \infty$, the second term goes to zero, but the first term remains

\vsp
Hence, correlation of the trees limits the benefit of averaging
\end{frame}

\begin{frame}
\frametitle{Sensitivity and specificity}
\begin{tabular}{lp{8cm}}
\smallCapGreen{Sensitivity:} & The proportion of times we label \alb{recession}, given that \alb{recession} is the correct answer. \\
\smallCapGreen{Specificity:} & The proportion of times we label \alb{no recession}, given that \alb{no recession} is the correct answer.  
\end{tabular}
\vsp

We can think of this in terms of hypothesis testing.  If
\[
H_0: \textrm{ no recession},
\]
then 
\vsp

\begin{tabular}{ll}
\smallCapGreen{Sensitivity:} & $P(\textrm{reject } H_0 | H_0 \textrm{ is false})$, [1 - $P$(Type II error)] \\
\smallCapGreen{Specificity:} & $P(\textrm{accept } H_0 | H_0 \textrm{ is true})$, [1 - $P$(Type I error)] \\
\end{tabular}
\end{frame}


\begin{frame}
\frametitle{Confusion matrix}
We can report our results in a matrix:
\vsp

\begin{tabular}{ll|cc}
                      & &\multicolumn{2}{c}{\alo{Truth}} \\
                      & & Recession  & No Recession \\
                      \hline
\alo{Our}               & Recession & (A) & (B)\\  
\alo{Predictions} & No Recession        & (C) &  (D)
\end{tabular}

\vsp 
The total number of each combination is recorded in the table.

\vsp 
The overall miss-classification rate is 
\[
\frac{\textrm{(B)} + \textrm{(C)}}{\textrm{(A)} + \textrm{(B)} + \textrm{(C)} + \textrm{(D)}}
=
\frac{\textrm{(B)} + \textrm{(C)}}{\textrm{total observations}}
\]

\vsp
What is the sensitivity/specificity? \pause

\script{Sensitivity is (A)/[(A) + (C)], Specificity is (D)/[(B) + (D)]}
\end{frame}

%
%\begin{frame}
%\frametitle{Confusion matrix: Reminder}
%Alternatively, we can report our results in a matrix:
%\vsp
%
%\begin{tabular}{ll|cc}
%                      & &\multicolumn{2}{c}{\alo{Truth}} \\
%                      & & Growth  & Recession   \\
%                      \hline
%\alo{Our}               & Growth & (A) & (B)\\  
%\alo{Predictions} & Recession        & (C) &  (D)
%\end{tabular}
%\vsp
%
%For each observation in the test set, we compare our prediction to the truth.
%
%\vsp 
%The total number of each combination is recorded in the table.
%
%\vsp 
%The overall miss-classification rate is 
%\[
%\frac{\textrm{(B)} + \textrm{(C)}}{\textrm{(A)} + \textrm{(B)} + \textrm{(C)} + \textrm{(D)}}
%=
%\frac{\textrm{(B)} + \textrm{(C)}}{n_{\textrm{test}}}
%\]
%\end{frame}

\begin{frame}[fragile]
\frametitle{Tree results: Confusion matrices}
\begin{tabular}{cc}
\parbox{.8cm}{\alo{Our} \\ \alo{Preds}}
% \alo{Predictions}
&
\begin{tabular}{ll|rr|r}
                                          &             &\multicolumn{2}{c}{\alo{Truth}} &\\
                                          &             & Growth  & Recession  & Mis-Class \\
                      \hline
\smallCapGreen{Null}  & Growth & 111 & 26  & \\  
                                          & Recession        & 0 & 0 & 18.9\% \\ 
                                          \hline
\smallCapGreen{Tree} & Growth & 99 & 3 & \\  
                                          & Recession        & 12 & 23 & 10.9\% \\
                      \hline
              \smallCapGreen{Random} & Growth & 102 & 5& \\  
 \smallCapGreen{Forest}     & Recession        &  9 &  21& 10.2\% \\
                      \hline
 \smallCapGreen{Bagging}  & Growth & 104 & 3 & \\  
                                               & Recession        &  7 &  23 & 7.3\% 
\end{tabular}
\end{tabular}
\vsp

\end{frame}


\begin{frame}[fragile]
\frametitle{Tree results: Sensitivity \& specificity}
\begin{table}
\begin{tabular}{l|rr}
                                          &    Sensitivity & Specificity \\
                      \hline
\smallCapGreen{Null}  &   0.000 & 1.000 \\
                                          &          & \\
                                          \hline
\smallCapGreen{Tree} & 0.884 & 0.891 \\
                                          &          & \\      
                      \hline
\smallCapGreen{Random} & 0.807 & 0.918 \\
 \smallCapGreen{Forest}    & \\
                      \hline
 \smallCapGreen{Bagging}  & 0.884 & 0.936 \\
                                                   & & \\
\end{tabular}
\end{table}

\end{frame}


\begin{frame}
\frametitle{Out-of-bag error estimation for bagging}

\begin{tabular}{ll|rr|r}
                                          &             &\multicolumn{2}{c}{\alo{Truth}} &\\
                                          &             & Growth  & Recession  & Miss-Class \\
                      \hline
 \smallCapGreen{OOB Bagging}  & Growth & 400 & 10 & \\  
                                               & Recession        &  21 &  46 & 6.5\% \\
                                               \\
 \smallCapGreen{Test Bagging}  & Growth & 104 & 3 & \\  
                                               & Recession        &  7 &  23 & 7.3\%                                                
\end{tabular}

%
%\begin{tabular}{cc}
%\parbox{1.4cm}{\alo{OOB} }
%\parbox{1.4cm}{\alo{Test}}
%% \alo{Predictions}
%&
%\begin{tabular}{ll|rr|r}
%                                          &             &\multicolumn{2}{c}{\alo{Truth}} &\\
%                                          &             & Growth  & Recession  & Mis-Class \\
%                      \hline
%\smallCapGreen{Null}  & Growth & 111 & 26  & \\  
%                                          & Recession        & 0 & 0 & 18.9\% \\ 
%                                          \hline
%\smallCapGreen{Tree} & Growth & 99 & 3 & \\  
%                                          & Recession        & 12 & 23 & 10.9\% \\
%                      \hline
%              \smallCapGreen{Random} & Growth & 102 & 5& \\  
% \smallCapGreen{Forest}     & Recession        &  9 &  21& 10.2\% \\
%                      \hline
% \smallCapGreen{Bagging}  & Growth & 104 & 3 & \\  
%                                               & Recession        &  7 &  23 & 7.3\% 
%\end{tabular}
%\end{tabular}

\end{frame}
%
%
%\begin{frame}
%\frametitle{Results of Bagging on recession data}
%%\alr{Same here: can you make the points bigger, solid?}
%\begin{figure}
%  \centering
%  \includegraphics[width=3in,trim=03 0 30 40,clip]
%  {../figures/recessionTrees2class.pdf} \\
%\end{figure}
%\end{frame}
%

\begin{frame}[fragile]
\frametitle{Random Forest in \alr{R}}
\begin{blockcode}
require(randomForest)
out.rf   = randomForest(X,Y,importance=T,mtry=p)
class.rf = predict(out.rf,X_0)
\end{blockcode}


\smallCapGreen{Notes:} 
\begin{itemize}
\item The \alr{importance} statement tells it to produce the variable importance measures
\item the \alr{mtry = p} tells \alr{randomForest} to consider all the covariates at each split

\script{This particular choice corresponds to bagging}
\item \alr{randomForest} also supports formulae 
\begin{blockcode}
out.rf   = randomForest(Y~.,data=X)
\end{blockcode}
However, it can take much longer to run
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Random Forest in \alr{R}}
\begin{blockcode}
> out.rf

Call:
randomForest(formula = Y~.,data = X, import = T, mtry = p)
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 56

        OOB estimate of  error rate: 7.33%
Confusion matrix:
    0  1 class.error
0 508 13  0.02495202
1  32 61  0.34408602
\end{blockcode}
\end{frame}


\begin{frame}[fragile]
\frametitle{Random Forest in \alr{R}}
\begin{blockcode}
#Permutation variable importance
> head(importance(out.rf,type=1))
           MeanDecreaseAccuracy
Alabama               3.7277511
Alaska                1.7941463
Arizona               2.9659623
Arkansas              0.8341577
California            7.2973572
#Mean decrease variable importance
> head(importance(out.rf,type=2))
           MeanDecreaseGini
Alabama           0.4551073
Alaska            1.6440170
Arizona           0.7025527
Arkansas          0.3503138
California        1.4616203

#variable importance plot:
varImpPlot(out.rf,type=2)
\end{blockcode}
\end{frame}

\begin{frame}
\frametitle{Additional random forest topics}
\smallCapGreen{Claim:} Random forest cannot overfit.
\vsp
This is and isn't true.  Write
\[
\hat{f}_{rf}^B = \frac{1}{B} \sum_{b=1}^B T(x;\Theta_b)
\]
where $\Theta_b$ characterizes the $b^{th}$ tree

\script{That is, the split variables, cutpoints of each node, terminal node values}
\vsp

Increasing $B$ does not cause Random forest to overfit, rather removes the
Monte-Carlo-like approximation error
\[
\hat{f}_{rf}(x) = \E_\Theta T(x,\Theta) = \lim_{B\rightarrow\infty}\hat{f}_{rf}^B
\]

\vsp

However, \alo{this limit can overfit the data}, the average of fully grown trees can result
in too complex of a model

\script{Note that Segal (2004) shows that a small benefit can be derived by stopping each tree short,
but thus induce another tuning parameter}
\end{frame}


\begin{frame}
\frametitle{Additional random forest topics}
Things I'd like to cover but may not 

\script{AKA possible short presentation topics}

\begin{itemize}
\item Variance and decorrelation effect (showing precisely how random forest may work/not work)
\item Introduction of noise covariates improves performance
\item Using \alo{subsampling} instead of bootstrap to generate trees

\script{This is an idea I had while writing this up.  I don't know if this exists, but it seems to work in many cases the bootstrap
doesn't and is easier to do theory (just use Hoeffding's inequalty for $U$-statistics)}
\item Adaptive nearest neighbors
\end{itemize}
\end{frame}

\end{document}
