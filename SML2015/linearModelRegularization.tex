\documentclass{beamer}
%\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{_defsAndPackages675notation}
\usepackage{_defsAndPackages675beamer}

\begin{document}

\title{\alg{Linear Methods for Regression:
Regularization}}
\subtitle{\classTitle}
%\author{\alg{Darren Homrighausen, PhD}}
%\institute{\classTitle}
\date{}



\begin{frame}
\maketitle
%\titlepage
%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=1in]{.../figures/CSU_logo2.eps}
%\end{figure}
%
\organization
%
\end{frame}


\begin{frame}[fragile]
\frametitle{Least squares}

\begin{figure}[!h]
\includegraphics[width=3in,trim=0 40 0 0,clip]{../figures/3dSquaredErrorOnly.pdf}
\end{figure}
\[
\hbeta_{LS} = \argmin_{ \beta\in\mathbb{R}^p} ||Y - \X  \beta||_2^2
\]

\end{frame}



\begin{frame}[fragile]
\frametitle{Regularization}
Another way to control bias and variance is through \alg{regularization} or
\alg{shrinkage}.  

\vsp
The idea is to make your
estimates of $\beta$ `smaller', rather than set them to zero 

{\scriptsize (which is what all subsets does)}

\vsp
One way to do this is called \alg{ridge regression}\footnote{Hoerl, Kennard  (1970)}:
\[
\hbetar{t} = \argmin_{ || \beta ||_2^2 \leq t} ||Y - \X  \beta||_2^2
\]
for any $t \geq 0$.  



\vsp
Compare this to \alg{least squares}

\[
\hbeta_{LS} = \argmin_{ \beta\in\mathbb{R}^p} ||Y - \X  \beta||_2^2
\]

\end{frame}



\begin{frame}
\frametitle{Geometry of ridge regression in $\mathbb{R}^2$}
\begin{figure}
  \centering
   \includegraphics[width=2in,trim=40 50 40 50,clip] {../figures/l_pBalls2ellipseAnnotated.pdf} 
   \includegraphics[width=2in,trim=0 0 0 0,clip]{../figures/3dSquaredErrorOnly.pdf}
\end{figure}
\end{frame}  

\begin{frame}[fragile]
\frametitle{Ridge regression}
An equivalent way to write
\begin{equation}
\hbetar{t} = \argmin_{ || \beta ||_2^2 \leq t} ||Y - \X  \beta||_2^2
\label{eq:1}
\end{equation}
is in the \alg{Lagrangian} form
\begin{equation}
\hbetar{\lambda} = \argmin_{ \beta} ||Y - \X  \beta||_2^2 + \lambda || \beta ||_2^2.
\label{eq:2}
\end{equation}
For every $\lambda'$ there is a unique $t'$ (and vice versa) that makes 
\[
\hbetar{\lambda'} = \hbetar{t'}
\]
\end{frame}

\begin{frame}[fragile]
\frametitle{Regularization and standardization}
The coefficient vector isn't invariant to rescaling. 

\vsp
If an intercept is included, do not penalize it:
\[
\min_{\beta_0,\beta} \sum_{i=1}^n (Y_i - \beta_0 + \beta^{\top}X_i)^2 + \lambda \norm{\beta}_2^2
\]

The usual way of addressing this in regression is:
\begin{itemize}
\item \alg{Standardize} \alo{all} covariates for which scale is meaningful:
\[
x_j \leftarrow \frac{(x_j - \textrm{mean}(x_j))}{\textrm{sd}(x_j)}
\]

\script{So, don't standardize indictor variables, for instance }
\item Standardize the response $Y \leftarrow Y - \textrm{mean}(Y)$
\item Don't include an intercept

\script{It would have been equal to $\textrm{mean}(Y)$}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Ridge regression}

Observe:
\begin{itemize}
\item $\lambda = 0$ (or $t = \infty$) makes $\hbetar{\lambda = 0} = \hbeta_{LS}$
\item Any $\lambda > 0$ (or $t <\infty$)  penalizes larger values of $ \beta$, effectively shrinking them.
\end{itemize}

Note: $\lambda$ and $t$ are known as \alg{tuning parameters} 

\script{Alternatively, hyper-parameters}

\vsp
However we think about it, we have produced a \alo{suite} of solutions
\[
\{\hbetar{\lambda} : \lambda \in [0,\infty)\}
\]
What do these solutions look like?

\end{frame}

\begin{frame}
\frametitle{Ridge regression path}
\begin{figure}
  \centering
   \includegraphics[width=3in] {../figures/beta_ridgePathNoCV.pdf} 
\end{figure}
\end{frame}  

\begin{frame}
\frametitle{Ridge regression}
\smallCapGreen{Reminder:} The least squares solution can be written:
\[
\hat\beta_{\textrm{LS}} = (\X^{\top} \X)^{\dagger} \X^{\top} Y.
\]

However, if $\rank(\X) < p$, then $\hat\beta_{\textrm{LS}}$ is not unique.  In fact, 
\[
\forall b \in \{b:\X b = 0\} 
\]
 $\hat\beta_{\textrm{LS}} + b$ is a valid least squares solution.  

\vsp
It turns out through differential calculus, we can write out the ridge regression solution as well:
\[
\hbetar{\lambda} = (\X^{\top} \X + \lambda I)^{-1} \X^{\top} Y 
\]

Quite similar.  However, the $\lambda$ can make all the difference..
\end{frame}


\begin{frame}
\frametitle{Regularization - Ridge Regression}

Using the \alo{SVD} $(\X = U D V^{\top})$, we can look even deeper. 
\begin{align*}
\hbeta_{\textrm{LS}} & = V D^{-1} U^{\top} Y  & = & \sum_{j=1}^p \v_j \left(\frac{1}{d_j} \right)\u_j^{\top}Y \\
\hbetar{\lambda} & = V (D^2 + \lambda I)^{-1} D U^{\top} Y & = & \sum_{j=1}^p \v_j \left( \frac{d_j}{d_j^2 + \lambda} \right)\u_j^{\top}Y.
\end{align*}
Similarly
\begin{align*}
\X\hbeta_{\textrm{LS}} & = U U^{\top} Y  & = & \sum_{j=1}^p \u_j \u_j^{\top}Y \\
\X\hbetar{\lambda} & = U D (D^2 + \lambda I)^{-1} D U^{\top} Y & = & \sum_{j=1}^p \u_j \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\u_j^{\top}Y.
\end{align*}

\textbf{$\Rightarrow$ Ridge shrinks the data by an additional factor of $\lambda$}.
\end{frame}


\begin{frame}[fragile]
\frametitle{Ridge Regression: A Bayesian approach }
Suppose we specify the likelihood as
\[
Y_i \sim N(X_i^{\top}\beta, \sigma^2)
\]
and put a prior distribution of $\beta \sim N(0,\tau^2I)$.

\vsp
Then we have the following posterior (making some conditional independence assumptions)
\[
p(\beta | Y, X, \sigma^2,\tau^2) \propto p(Y | X ,\beta, \sigma^2) p(\beta | \tau^2).
\]
After kernel matching, we find that the posterior mode/mean is
\[
\hbetar{\lambda = \sigma^2/\tau^2}
\]
\end{frame}


\begin{frame}
\frametitle{Ridge regression in a new space }
Note the matrix identity
\[
(A - BC^{-1}E)^{-1}BC^{-1} = A^{-1} B (C -  EA^{-1}B)^{-1}
\]
\script{Henderson, Searle (1980), equation (13)}

Then,
\[
\hbetar{\lambda} = (\X^{\top} \X + \lambda I)^{-1} \X^{\top} Y = \X^{\top}( \X\X^{\top} + \lambda I)^{-1}Y
\]
\end{frame}

\begin{frame}
\frametitle{Ridge  in a new space: Computations }
The ridge solution solves either the \alo{normal equations}
\[
(\X^{\top} \X + \lambda I)\hat\beta =  \X^{\top} Y 
\]
or the \alo{adjoint problem}
\[
\X^{\top}( \X\X^{\top} + \lambda I)^{-1}Y
\]
The `heavy lifting' in each case is done with the inversion

\vsp
\begin{itemize}
\item $\X^{\top} \X \in \R^{p \times p} \Longrightarrow$ takes $p^3$ computations, $p^2$ space 
\item $ \X \X^{\top}\in \R^{n \times n} \Longrightarrow$ takes $n^3$ computations, $n^2$ space
\end{itemize}

\smallCapGreen{Conclusion:} Depending on the relative size of $n$ and $p$, this could be substantial
savings

\vsp
However, a much deeper realization is possible..
\end{frame}



\begin{frame}
\frametitle{(Kernel) ridge regression}
Suppose we want to predict at $X$, then 
\[
\hat{f}(X) = X^{\top}\hbetar{\lambda} =  X^{\top}\X^{\top}( \X\X^{\top} + \lambda I)^{-1}Y
\]
Also,
\[
\X\X^{\top} = 
\begin{bmatrix}
\langle X_1, X_1 \rangle & \langle X_1, X_2 \rangle & \cdots & \langle X_1, X_n \rangle \\
& \vdots && \\
\langle X_n, X_1 \rangle & \langle X_n, X_2 \rangle & \cdots & \langle X_n, X_n \rangle
\end{bmatrix}
\]
and
\[
X^{\top}\X^{\top} = [\langle X, X_1 \rangle,  \langle X, X_2 \rangle, \cdots, \langle X, X_n \rangle]
\]
where $\langle X,X' \rangle = X^{\top}X'$ is the Euclidean inner product.

\vsp
If we transform $X_i \mapsto \Phi(X_i)$, and the range of $\Phi$ is equipped with an inner product, we can use
$\langle \Phi(X_i), \Phi(X_{i'}) \rangle$

\vsp
Inserting $\Phi$ is known as \alg{kernelization} or a \alg{kernel trick}
\end{frame}


\begin{frame}
\frametitle{(Kernel) ridge regression}
\smallCapGreen{Example:} Suppose $X = (\textrm{income},\textrm{height})^{\top}$

\vsp Then we could specify the map 
\[
\Phi(X)^{\top} = (\textrm{income},\textrm{height}, \textrm{income}*\textrm{height},\textrm{income}^2,\textrm{height}^2)
\]
The induced \alo{feature matrix} is then
\[
\X = 
\begin{bmatrix}
\Phi(X_1) \\
 \vdots  \\
\Phi(X_n)
\end{bmatrix}
\in
\R^{n \times 5}
\]
\end{frame}

\begin{frame}
\frametitle{(Kernel) ridge regression}
Ordinarily, this would mean we need to solve the \alo{normal equation} inversion for $p = 5$
\begin{itemize}
\item $\X^{\top} \X \in \R^{p \times p} \Longrightarrow$ takes $p^3$ computations, $p^2$ space 
\end{itemize}
\vsp

However, using the \alo{kernel trick} we can solve instead
\begin{itemize}
\item $ \X \X^{\top}\in \R^{n \times n} \Longrightarrow$ takes $n^3$ computations, $n^2$ space
\end{itemize}
which is fixed in $p$

\vvvsp
\smallCapGreen{Implication:} We can add essentially arbitrary nonlinearity without paying higher computational, storage
 cost!
 
 \vvsp
 We will return to this again with \alg{support vector machines} (SVM)
\end{frame}

\transitionSlide{Ridge in practice}

\begin{frame}[fragile]
\frametitle{Ridge Regression: The tuning parameter}
We can use a degrees of freedom based risk estimator to choose $\lambda$

\vsp
The degrees of freedom of $\hbetar{\lambda}$ can be seen to be
\[
\df = \tr \left[\X(\X^{\top}\X + \lambda I)^{-1} \X^{\top}\right] = \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda}
\]
\script{As $\lambda \rightarrow 0$, we get the number of parameters}

A common, classic choice is \alg{generalized cross-validation} (GCV), which has the form:
\[
\textrm{GCV}(\hat\beta) = \frac{\hat\P \ell_{\hat{\beta}}}{(1-\df(\hat{\beta})/n)^2}
\]

\script{Golub, Heath, Wahba (1979)}

\vsp
Note that this looks a lot like AIC with unknown variance, but with $\log(1- \df{}/n)$ as penalty
\end{frame}

\begin{frame}[fragile]
\frametitle{Ridge Regression: The tuning parameter}
To see this last claim, observe

\begin{align*}
\log\left(\textrm{GCV}(\hat\beta)\right) &\propto \log(\train) - 2\log(1-\df(\hat{\beta})/n)\\
\textrm{\smallCapGreen{versus}} & \\
\textrm{AIC}(\hat\beta) & \propto \log(\train) +  2n^{-1}\df(\hat{\beta})
\end{align*}

\end{frame}

\begin{frame}[fragile]
\frametitle{Ridge Regression: The tuning parameter}
Nowadays, using $K$-fold cross-validation is common

\vsp
Think of $CV_K$ as a function of $\lambda$, and pick its \alo{minimum}:
\[
\hat\lambda = \argmin_{\lambda \geq 0} CV_K(\lambda)
\]
\vsp

Now, we report $\hbetar{\hat\lambda}$ as our estimator
\end{frame}


\begin{frame}[fragile]
\frametitle{Ridge Regression: Computation}
There are several ways to compute ridge regression

\vsp
We can follow any conventional least squares solving technique (i.e.: QR factorization, Cholesky Decomposition, SVD,...):
\[
(\X^{\top}\X + \lambda I)\beta = \X^{\top}Y
\]
\vsp

Alternatively, we can actually solve it using \alr{lm} in \alr{R} if we make the following augmentation

\[
\tilde{Y} = 
\begin{bmatrix}
Y_1 \\
\vdots \\
Y_n \\
0 \\
\vdots \\
0
\end{bmatrix}
\in
\R^{n + p}
\textrm{ and }
\tilde{\X} = 
\begin{bmatrix}
\X \\
\sqrt{\lambda} I 
\end{bmatrix}
\]
\end{frame}

\begin{frame}[fragile]
\frametitle{Ridge Regression in R}

We will concentrate on a slightly more complicated way, as it will make  things easier later.

\begin{blockcode}
install.packages('glmnet')
library(glmnet)
ridge.out = cv.glmnet(x=X,y=Y,alpha=0)
\end{blockcode}

\end{frame}

\begin{frame}[fragile]
\frametitle{Ridge Regression: CV plot}
\begin{blockcode}
X = as.matrix(X)
ridge.out = cv.glmnet(x=X,y=Y,alpha=0)

plot(ridge.out$lambda,ridge.out$cvm,
       xlab='lambda',ylab='CV error',main='Ridge',type='l')
abline(v=ridge.out$lambda[which.min(ridge.out$cvm)])
\end{blockcode}
%min.lambda = min(ridge.out$lambda)
%lambda.new = seq(min.lambda*10,min.lambda*.001,length=100)
%ridge.out = cv.glmnet(x=X,y=Y,alpha=0,lambda=lambda.new)

\begin{figure}
  \centering 
  \includegraphics[width=1.6in,trim=0 0 0 25,clip] {../figures/ridgeCV} 
\end{figure}    

\end{frame}

\begin{frame}
\frametitle{Ridge regression path}
\begin{figure}
  \centering
   \includegraphics[width=3in] {../figures/beta_ridgePath.pdf} 
\end{figure}
\end{frame}  


%\begin{frame}[fragile]
%\frametitle{Ridge Regression: Using the $\lambda$}
%Here is the chosen fit (along with some previous methods):
%\begin{blockcode}
%#for predictor coefficient estimates
%ridge.out$glmnet.fit$beta[,which.min(ridge.out$cvm)] 
%#for intercept
%ridge.out$glmnet.fit$a0[which.min(ridge.out$cvm)] 
%\end{blockcode}
%\begin{tabular}{lrrr}
%Variable & Ridge & Full Linear Model & Forward and Backward\\ 
%%intercept & -0.017  & 0.181561 &  0.4947\\
%lcavol   &  0.474  &  0.564341  & 0.543 \\
%lweight  &   0.597 &  0.622020 & 0.588\\
%age &  -0.015 &   -0.021248  & -0.016 \\
%lbph & 0.083   & 0.096713 & 0.101\\
%svi & 0.667  & 0.761673 & 0.715 \\
%lcp &  -0.025 & -0.106051 & 0\\
%gleason & 0.066 & 0.049228 & 0  \\
%pgg45 &  0.003 & 0.004458 & 0\\
%\end{tabular}
%\end{frame}
%
%\begin{frame}[fragile]
%\frametitle{Ridge regression and multicollinearity}
%Multicollinearity is a phenomenon in which a combination of predictor variables is extremely similar to
%another predictor variable. Some comments:
%\begin{itemize}
%\item A better term that is sometimes used is \alg{$\X$ is ill-conditioned}
%\item It means that one of its columns is nearly (or exactly) a linear combination of other columns.  This is sometimes known
%as `(numerically) rank-deficient'.
%\item If $\X = U D V^{\top}$ is ill-conditioned, then some elements of $D$ are \alo{nearly zero} 
%
%{\scriptsize (remember, $D$ is a diagonal matrix with decreasing entries)}
%\item If we form $\hbeta_{LS} = (\X^{\top}\X)^{-1}\X^{\top}Y = V D^{-1} U^{\top} Y$, then we see that the small
%entries of $D$ are now huge (due to the inverse).  This in turn creates a \alo{huge variance }
%
%{\scriptsize ($\mathbb{V} \hbeta_{LS} =  (\X^{\top}\X)^{-1} = V D^{-2} V^{\top}$)}
%\end{itemize}
%\end{frame}
%
%\begin{frame}[fragile]
%\frametitle{Ridge regression and multicollinearity}
%Ridge Regression fixes this problem by preventing the division by a \alo{near zero number}  
%
%\vsp
%\emphasis{9cm}{Example:}{If $a$ is a really small number, and $\lambda > 0$ is another number, then
%\[
%\frac{1}{a} \approx \infty \quad \textrm{while} \quad \frac{1}{a + \lambda} \textrm{ is much smaller.}
%\]
%To wit, $1/0.0001 = 10,000$, while $1/(0.0001 + 0.1) \approx 10$}
%
%\vsp
%
%\emphasis{7cm}{Conclusion:}{
%
%$(\X^{\top}\X)^{-1}$ can be really unstable, while $(\X^{\top}\X + \lambda I)^{-1}$ is not.}
%\end{frame}
%
%\begin{frame}[fragile]
%\frametitle{Ridge regression and multicollinearity: Example}
%Consider the example of predicting blood pressure from a person's weight and body surface area.
%\begin{blockcode}
%blood = read.table('../data/bloodpress.txt',header=T)
%
%Y = blood$BP
%weight = blood$Weight #persons weight
%bsa = blood$BSA
%
%outBoth = lm(Y~bsa+weight)
%summary(outBoth)
%outBSA = lm(Y~bsa)
%summary(outBSA)
%outWeight = lm(Y~weight)
%summary(outWeight)
%\end{blockcode}
%\end{frame}
%
%\begin{frame}[fragile]
%\frametitle{Ridge regression and multicollinearity: Example}
%\begin{blockcode}
%lm(formula = Y ~ bsa + weight)
%            Estimate Std. Error t value Pr(>|t|)
%(Intercept)    5.653      9.392   0.602    0.555
%bsa           11.663     12.125   0.962    0.350
%weight        -4.793      6.232  -0.769    0.452
%
%lm(formula = Y ~ bsa)
%            Estimate Std. Error t value Pr(>|t|)    
%(Intercept)   2.7971     8.5284   0.328    0.747    
%bsa           2.3389     0.1792  13.052 1.29e-10 ***
%
%lm(formula = Y ~ weight)
%            Estimate Std. Error t value Pr(>|t|)    
%(Intercept)  2.20531    8.66333   0.255    0.802    
%weight       1.20093    0.09297  12.917 1.53e-10 ***
%
%\end{blockcode}
%\end{frame}
%
%\begin{frame}[fragile]
%\frametitle{Ridge Regression and Multicollinearity: Example}
%\begin{blockcode}
%out = cv.glmnet(x=cbind(weight,bsa),y=Y,alpha=0,nfolds=4,
%                            lambda=(1:100)/100)
%out$lambda[which.min(out$cvm)]
%[1] 0.35
%> out$glmnet.fit$beta[,which.min(out$cvm)]
%  weight      bsa 
%0.574093 1.146265 
%> out$glmnet.fit$a0[which.min(out$cvm)]
%     s65 
%6.059681 
%\end{blockcode}
%\end{frame}
%


\begin{frame}
\frametitle{Can we get the best of both worlds?}
To recap:
\begin{itemize}
\item Forward, backward, and all subsets regression offer good tools for model selection.

\script{but the optimization problem is nonconvex}

\item Ridge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.  

{\scriptsize (problem is convex, but doesn't do model selection)}
\end{itemize}

\begin{table}
\begin{tabular}{ll}
\smallCapGreen{Ridge regression} & $\min ||\mathbb{Y}-\X\beta||_2^2 \textrm{ subject to } ||\beta||_2^2 \leq t$ \\
& \\
\smallCapGreen{Best linear } &  $\min || \mathbb{Y}-\X\beta||_2^2 \textrm{ subject to } ||\beta||_0 \leq t$ \\
\smallCapGreen{regression model} & \\
& $(||\beta||_0 = $ the number of nonzero elements in $\beta)$
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[fragile]
\frametitle{An intuitive idea}
\begin{table}
\begin{tabular}{ll}
\smallCapGreen{Ridge regression} & $\min ||\mathbb{Y}-\X\beta||_2^2 \textrm{ subject to } ||\beta||_2^2 \leq t$ \\
& \\
\smallCapGreen{Best linear } &  $\min || \mathbb{Y}-\X\beta||_2^2 \textrm{ subject to } ||\beta||_0 \leq t$ \\
\smallCapGreen{regression model} & \\
& $(||\beta||_0 = $ the number of nonzero elements in $\beta)$
\end{tabular}
\end{table}

\vsp

\begin{tabular}{lll}
%                & \multicolumn{2}{c}{Best linear regression model} \\
                                                 & \smallCapGreen{Best linear}                                 & \smallCapGreen{Ridge} \\
                                                 &                     \smallCapGreen{regression model} & \smallCapGreen{regression} \\                                                 
  \alo{Computationally Feasible?} & No                                                & Yes     \\ 
  \alo{Does Model Selection?}     & Yes                                               & No     
\end{tabular}
\vsp

Can we `interpolate' $|| \beta ||_2$ and $||\beta||_0$ to find a method that does both?
\end{frame}

\begin{frame}[fragile]
\frametitle{Geometry of regularization in $\mathbb{R}^2$: {\footnotesize Convexity}}
\begin{table}
\begin{tabular}{ccc}
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls0.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBallsPoint5.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBallsPoint75.pdf} \\  
\textcolor<2>{bluemain}{$||\beta||_{0} \leq t$} &  
\textcolor<2>{bluemain}{$||\beta||_{\frac{1}{2}} \leq t$} & 
\textcolor<2>{bluemain}{$||\beta||_{\frac{3}{4}} \leq t$} \\  
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls1.pdf}  &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls1Point5.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls2.pdf} \\  
\textcolor<3>{redmain}{$||\beta||_{1} \leq t $} &  
\textcolor<3>{redmain}{$||\beta||_{\frac{3}{2}} \leq t $} & 
\textcolor<3>{redmain}{$||\beta||_2 \leq t$ }
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[fragile]
\frametitle{Geometry of regularization in $\mathbb{R}^2$: {\footnotesize Model selection}}
\begin{table}
\begin{tabular}{ccc}
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls0ellipse.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBallsPoint5ellipse.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBallsPoint75ellipse.pdf} \\  
\textcolor<2>{redmain}{$||\beta||_{0} \leq t$} &  
\textcolor<2>{redmain}{$||\beta||_{\frac{1}{2}} \leq t$} & 
\textcolor<2>{redmain}{$||\beta||_{\frac{3}{4}} \leq t$} \\  
  \includegraphics<-3>[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls1ellipse.pdf}  &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls1Point5ellipse.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls2ellipse.pdf} \\  \textcolor<2>{redmain}{$||\beta||_{1} \leq t $} &  
\textcolor<3>{bluemain}{$||\beta||_{\frac{3}{2}} \leq t $} & 
\textcolor<3>{bluemain}{$||\beta||_2 \leq t$ }
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[fragile]
\frametitle{Geometry of regularization in $\mathbb{R}^2$: {\footnotesize Both}}
\begin{table}
\begin{tabular}{ccc}
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls0ellipse.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBallsPoint5ellipse.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBallsPoint75ellipse.pdf} \\  
$||\beta||_{0} \leq t$ &  
$||\beta||_{\frac{1}{2}} \leq t$ & 
$||\beta||_{\frac{3}{4}} \leq t$ \\  
    \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls1ellipseRed.pdf}  &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls1Point5ellipse.pdf} &
  \includegraphics[width=1.3in,trim=40 50 40 50,clip] {../figures/l_pBalls2ellipse.pdf} \\  
\textcolor{redmain}{$||\beta||_{1} \leq t $} &  
$||\beta||_{\frac{3}{2}} \leq t $ & 
$||\beta||_2 \leq t$ 
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[fragile]
\frametitle{Summary}
\begin{table}
\begin{tabular}{lccr}
                                          & \smallCapGreen{Convex?} & \smallCapGreen{Corners?} &\\
                                          \\
$||\beta||_0$                    & \textcolor{bluemain}{No}     & \textcolor{redmain}{Yes} &\\
$||\beta||_{\frac{1}{2}}$  & \textcolor{bluemain}{No}     & \textcolor{redmain}{Yes} & \\
$||\beta||_{\frac{3}{4}}$  & \textcolor{bluemain}{No}     & \textcolor{redmain}{Yes} & \\
\\
$||\beta||_1$                    & \textcolor{redmain}{Yes}    & \textcolor{redmain}{Yes} & \checkmark\\
\\
$||\beta||_{\frac{3}{2}}$  & \textcolor{redmain}{Yes}      & \textcolor{bluemain}{No} &\\
$||\beta||_2$                    & \textcolor{redmain}{Yes}      & \textcolor{bluemain}{No} &
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[fragile]
\frametitle{The best of both worlds: $||\beta||_{1}$}
\begin{figure}
\centering
  \includegraphics[width=2in,trim=40 100 40 50,clip] {../figures/l_pBalls1ellipse.pdf}  
  \end{figure}
  \vsp
  
  This regularization set...
  \begin{itemize}
  \item[]  ... is convex (computationally efficient)
  \item[]  ... has corners (performs model selection)
  \end{itemize}
\end{frame}




\transitionSlide{$\ell_1$-regularized regression}

\begin{frame}[fragile]
\frametitle{$\ell_1$-regularized regression}
Related methods are known as 
\begin{itemize}
\item \smallCapGreen{lasso:} The covariates are recorded
\item \smallCapGreen{basis pursuit:} The covariates are frames comprised of various bases
\item \smallCapGreen{compressed sensing:} The covariates are random draws from some distribution
\end{itemize}
\vsp

The estimator satisfies
\[
\hat\beta_{lasso}(t) = \argmin_{ ||\beta||_1 \leq t}  ||\mathbb{Y}-\X\beta||_2^2 
\]

In its corresponding Lagrangian dual form:
\[
\hat\beta_{lasso}(\lambda) = \argmin_{\beta} ||\mathbb{Y}-\X\beta||_2^2 + \lambda ||\beta||_1
\]

\script{Note that if $\rank(X) < p$, then the objective function is not strictly convex.  There are now an infinite number of possible
lasso solutions. (all must have the same fitted value and $||\cdot||_1$)}
\end{frame}

\begin{frame}
\frametitle{Lasso regression path}
\begin{table}
  \centering
  \begin{tabular}{cc}
     \includegraphics[width=2.2in] {../figures/beta_ridgePathNoCV.pdf}  &
   \includegraphics[width=2.2in] {../figures/beta_lassoPathNoCV.pdf}  \\
   Ridge & Lasso
\end{tabular}
\end{table}
\end{frame}  




%\begin{frame}[fragile]
%\frametitle{Can We Get the Best of Both Worlds? Yes!}
%Is there some middle ground?  Yes, there is.  Let's use the estimator that satisfies:
%\[
%\min ||Y-\X\beta||_2^2 \textrm{ subject to } ||\beta||_1 \leq t
%\]
%where recall $||\beta||_1 = \sum_{j=1}^p |\beta_j|$.  
%\vspace{.1in}
%
%Here, informally, we can think of $||\beta_j||_1$ being `between' the constraint 'number of nonzero entries in $\beta \leq t$'
%and the constraint $||\beta||_2^2 \leq t$.
%\begin{itemize}
%\item This new optimization problem is still convex (good/efficient numerical properties).
%\item Has many good theoretical properties.
%\item Can do model selection!
%\end{itemize}
%\end{frame}
%
%\begin{frame}[fragile]
%\frametitle{How Can This be True?}
%\begin{figure}
%  \centering 
%  \includegraphics[width=4in] {../figures/l1l2} 
%  \caption*{Left: The $||\beta||_1$ penalty.  Right: The $||\beta||_2^2$ penalty.  Note how the intersection of the ellipse and
%  the blue area happens at a coordinate axis in the left figure.}
%\end{figure}    
%\end{frame}
%
%\begin{frame}[fragile]
%\frametitle{The Lasso}
%We can write the constrained optimzation problem:
%\[
%\min ||Y-\X\beta||_2^2 \textrm{ subject to } ||\beta||_1 \leq t
%\]
%in its corresponding Lagrangian dual form and define the Lasso as:
%\[
%\hat\beta_{\textrm{Lasso},\lambda} = \argmin_{\beta} ||Y-\X\beta||_2^2 + \lambda ||\beta||_1.
%\]
%Of course, just like in Ridge, we need a way of choosing this smoothing parameter.  We'll just use
%cross-validation again (although this is still an area of active research).
%\end{frame}


\begin{frame}[fragile]
\frametitle{The lasso in R: glmnet}
Luckily, we already know how to lasso.  
\vsp

Just change the `\alr{alpha =0}' to `\alr{alpha =1}', and you're lassoing.
\begin{blockcode}
lasso.out = glmnet(x=as.matrix(X),y=Y,alpha=1)
#Note: glmnet automatically scales X
\end{blockcode}

\vsp
\alr{glmnet} uses \alg{gradient descent} to quickly fit the lasso solution

\vsp  
It can...
\begin{itemize}
\item handle other likelihoods than Gaussian
\item supports/exploits sparse matrices (e.g. for text processing)
\item use warm restarts for the grid of $\lambda$ to produce more stable fits/faster computations
\end{itemize}
\script{See (Friedman et al. (2007) for details)}
\end{frame}
%
%\begin{frame}
%\frametitle{Terminology}
%In computer science, we use the following \smallCapGreen{terms}.  
%\begin{itemize}
%\item \smallCapGreen{Cost function: } The objective function for an optimization problem ($J$)
%\item \smallCapGreen{Hypothesis: } This is the restriction, or constraint, for the optimization ($h$)
%\item \smallCapGreen{Parameter: } The argument for the cost function ($\theta$)
%\end{itemize}
%\vsp
%\emphasis{7cm}{Example:}{}
%\[
%\underbrace{\min_{f \in \F} \hat P \ell_f}_{\textrm{Statistics}} = \underbrace{\min_{\theta} J(h_\theta) = \min_{\theta} J(\theta) }_{\textrm{Computer science}}
%\]
%\end{frame}

\begin{frame}
\frametitle{Optimality conditions: Review}
\begin{align}
\textrm{minimize } & F(x) \\
\textrm{subject to } & x \in \mathbb{R}^p 
\end{align}
%Turn the optimization from a geometric one to an algebraic one: look for a point for which the gradient vanishes
\[
\textrm{Search for $x_*$ such that } \nabla F|_{x_*} = 0
\]

\vsp
\begin{itemize}
\item Turns a geometric problem into an algebraic problem: solve for the point where the gradient vanishes
\item Is necessary for optimality of $x_*$.  Is sufficient if $F$ is convex and smooth.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Gradient descent: Intuition}
A summary:
\begin{enumerate}
\item Start with some initial $x^0$
\item Propose $x$ to reduce $F(x)$
\item Alternate between \textcolor{bluemain}{1.} and \textcolor{bluemain}{2.} until the objective function doesn't change (much).
\end{enumerate}

Algorithmically, the implementations tend to look like
\[
x[k+1] \leftarrow x[k] + \alpha_k v[k],
\]
where 
\begin{itemize}
\item $x[k]$ is the current value of the minimizing parameter
\item $v[k]$ is a direction that  (hopefully) reduces $F$
\item $\alpha_k$ is a relaxation term.
\end{itemize}
\end{frame}
%
%\begin{frame}
%\frametitle{Gradient descent: Different flavors}
%There are many different variations on the same theme, with slightly different details
%
%\begin{itemize}
%\item \smallCapGreen{Conjugate} or  \smallCapGreen{Accelerated}
%\item \smallCapGreen{Stochastic} 
%\item \smallCapGreen{Projected} or  \smallCapGreen{Proximal}
%\end{itemize}
%
%\end{frame}

%\begin{frame}
%\frametitle{Other optimization techniques}
%We won't directly be covering other common optimization techniques, which include
%\begin{itemize}
%\item \smallCapGreen{Expectation/minimization (EM)}
%\item \smallCapGreen{Simulated annealing}* 
%\item \smallCapGreen{Genetic algorithms}*
%\item \smallCapGreen{Broyden-Fletcher-Goldfarb-Shanno (BFGS)}
%\item \smallCapGreen{Newton's method}
%\end{itemize}
%
%\vfill
%*well suited to highly nonconvex programs
%\end{frame}


\begin{frame}
\frametitle{Gradient descent}
Assume $\exists x_* \in D$ such that $\nabla F|_{x_*} = 0$

\vsp
Define the map
\[
\psi(x) = x - \alpha \nabla F|_{x} 
\]
\script{Recall the general form
$x[k+1] \leftarrow x[k] + \alpha_k v[k]$}

\vsp

If $\psi$ is \alg{contractive}, ie
\[
||\psi(x) - \psi(x')|| \leq c ||x - x'||
\]
where $c \in [0,1)$, then...
\vsp

\emphasis{3cm}{Gradient descent is guaranteed to converge}

\end{frame}

\begin{frame}
\frametitle{Gradient descent: Convergence proof}
\begin{align}
||x[k+1] - x_*||  
& = 
||x[k] - \alpha \nabla F|_{x}   - x_*||  \\
& = || \psi(x[k]) - \psi(x_*) || \\
\label{eq:linear}
& \leq c ||x[k] - x_* || \\
& \vdots \\
& \leq c^{k+1} ||x[0] - x_* || \\
\end{align}
\script{This means we get exponential convergence\footnote{Optimization people call this linear convergence due
to equation (\ref{eq:linear})}}

\vsp
\emphasis{8cm}{Important fact:}{If $F$ is $2\times$ differentiable, contractivity means $F$ is convex on $D$}

\end{frame}


\begin{frame}[fragile]
\frametitle{Gradient descent example}
If we look at multiple regression via least squares we get:

\begin{align*}
\min_\beta \norm{Y- \X\beta}_2^2 
& 
\Rightarrow \frac{\partial}{\partial \beta_j} \norm{Y- \X\beta}_2^2 \\
& =
\frac{\partial}{\partial \beta_j} \sum_{i=1}^n (Y_i - X_i^{\top}\beta)^2 \\
& =
2\sum_{i=1}^n (Y_i - X_i^{\top} \beta)X_{ij} 
\end{align*}
Hence, we will cycle over $j$ and make the update $k = 1, \ldots, K$ iterations:
\[ 
\hat\beta_j^{k+1} = \hat\beta_j^{k} - \alpha \sum_{i=1}^n (Y_i - X_i^{\top} \hat\beta^{k})X_{ij}
\]

\end{frame}
\begin{frame}[fragile]
\frametitle{Gradient descent example}

\begin{figure}[!h]
\includegraphics[width=3.25in,trim=0 40 0 0,clip]{../figures/3dSquaredErrorOnly.pdf}
\caption*{With $RSS = \norm{Y - \X\beta}_2^2$ for $p = 2$}
\end{figure}

\end{frame}

\begin{frame}[fragile]
\frametitle{The lasso in R: LARS}

Alternatively, the \alr{lars} package exploits the fact that the coefficient 
profiles are piecewise linear, which leads to an algorithm with the same computational
cost as the full least-squares fit on the data 

\script{See Osborne et al. (2000) for details on the convex optimization, Efron et al. (2004) for
the LARS algorithm}
\end{frame}

\begin{frame}[fragile]
\frametitle{Choosing the tuning parameter for lasso}
Of course, just like in Ridge, we need a way of choosing this tuning parameter.  
\vsp

We can just use \alo{cross-validation} again, though this is still an area of active research:

\vsp
\begin{itemize}
\item[] {\footnotesize Homrighausen, D. and McDonald, D.J. \emph{Leave-one-out cross-validation is risk consistent for lasso},  Machine Learning}
\item[] {\footnotesize Homrighausen, D. and McDonald, D.J. \emph{Risk consistency of cross-validation for lasso-type procedures},  Journal of Machine Learning Research}
\item[] {\footnotesize Homrighausen, D. and McDonald, D.J. \emph{The lasso, persistence, and cross-validation}, 
(2013) International Conference on Machine Learning, JMLR  28(3), 1031--1039.}
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Choosing the tuning parameter for lasso}
For cross-validation, the heavy lifting has been done for us
\vsp

\begin{blockcode}
cv.glmnet(x=as.matrix(X),y=Y,alpha=1)

cv.lars(x=as.matrix(X),y=Y,type='lasso')
\end{blockcode}
Note that for the grid $\lambda$, we need only look over the interval $\left[0,||\X^{\top} Y||_\infty \right)$

\vsp
A grid of $t$ has a similar restriction $[0, t_0)$, where
\[
t_0 = \min_{ \{b : \X b = 0\} } || \hat\beta_{\textrm{LS}}  + b||_1
\]

\end{frame}

\begin{frame}[fragile]
\frametitle{The lasso in R}
\begin{figure}
  \centering 
  \includegraphics[width=3in] {../figures/lassoCV.pdf} 
\end{figure} 
\end{frame}

\begin{frame}
\frametitle{Lasso regression path}
\begin{figure}
  \centering
   \includegraphics[width=3in] {../figures/beta_lassoPath.pdf} 
\end{figure}
\end{frame}  

\begin{frame}
\frametitle{Comparison: Regression path}
\begin{figure}
  \centering
   \includegraphics[width=2.2in] {../figures/beta_ridgePath.pdf}   
   \includegraphics[width=2.2in] {../figures/beta_lassoPath.pdf} 
   \caption*{Vertical line at minimum CV tuning parameter}
\end{figure}

\end{frame}  
\begin{frame}[fragile]
\frametitle{Comparison of \alr{lars} and \alr{glmnet}}

There are two main problems with \alr{glmnet}
\begin{itemize}
\item In practice, the $\lambda$ interval looks like $\left[\epsilon||\X^{\top} Y||_\infty,||\X^{\top} Y||_\infty \right)$ for a small $\epsilon$. Sometimes, this results in finding a boundary solution.
\item The iterative nature sometimes results in bad coefficient vectors (such as having more than $\min\{n,p\}$ nonzero 
coefficients, which is impossible\footnote{This is not quite true (Tibshirani (2013), Lemma 13).  However, see Lemma 15 in same paper: For any $\X,\lambda$ and {\it almost} all $Y$, the column space of $\X_\mathcal{S}$ is the same for every $\mathcal{S}$,
where $\mathcal{S} = \{j : |\hat\beta_{\lambda,j}| > 0\}$})
\end{itemize}
\vsp

There are two main problems with \alr{lars}
\begin{itemize}
\item It is slow(er)
\item It doesn't support other likelihoods
\end{itemize}
\end{frame}
%\begin{frame}[fragile]
%\frametitle{Alternative method for choosing $\lambda$: Scaled sparse regression}
%
%Scaled sparse regression (Sun, Zhang (2012)).  Uses the idea that we know that the optimal
%$\lambda$ scales like $\sigma^2$.  Form the penalized joint likelihood:
%\[
%L_{\lambda_0}(\beta,\sigma) = \frac{\hat{\P}\ell_{\beta} }{2n\sigma} + (1-a)\sigma/2 + \lambda_0 ||\beta||_1
%\]
%Alternatively minimize $L$ in $\beta$ and $\sigma$ via:
%\begin{enumerate}
%\item $\hat{\sigma} \leftarrow ||Y - \X \hat{\beta}_{old} ||_2^2/\sqrt{n(1-a)}$
%\item $\lambda \leftarrow \hat\sigma \lambda_0$
%\item $\hat\beta \leftarrow \hat\beta_{\lambda}$
%\item If $||\hat\beta -  \hat{\beta}_{old}||$ is not small, set $\hat\beta_{old} \leftarrow \hat\beta$ and return to \textcolor{bluemain}{1.}
%\end{enumerate}

%\end{frame}

%\begin{frame}[fragile]
%\frametitle{Alternative method for choosing $\lambda$: Consistent cross validation}
%
%\end{frame}
\begin{frame}[fragile]
\frametitle{Flavors of lasso}
\begin{itemize}
\item Grouped lasso (Yuan and Lin (2007), Meier et al. (2008)), where variables are included
or excluded in groups.
\item Refitted lasso (e.g. Lederer 2013).  Takes the estimated model from lasso and fits the full
least squares solution on selected covariates (less bias, more variance).
\item Dantzig selector (Candes, Tao (2007)), a slightly modified version of the lasso
\item The elastic net (Zou, Hastie (2005)), generally used for correlated variables that
combines a ridge/lasso penalty.  Included in \alr{glmnet}.  Fixes non-uniqueness problem
of lasso (although, see Tibshirani (2013)).  
\item SCAD (Fan and Li (2005)), a non-convex version of lasso that adds a more severe variable selection penalty
\item $\sqrt{\textrm{lasso}}$ (Belloni et al. (2011)), claims to be tuning parameter free (but isn't).  Uses $||\cdot||_2$
instead of $||\cdot||_2^2$ for the loss.
\item Generalized lasso (Tibshirani, Taylor (2011)).  Adds various additional penalty matrices to the penalty term (ie: 
$||D\beta||_1$).  
\end{itemize}

\end{frame}
%
%\transitionSlide{Ridge theory}
%
%\begin{frame}[fragile]
%\frametitle{Set-up: assuming a linear model}
%Let $Y_i = X_i^{\top} \beta + \epsilon_i$, for $i=1,\ldots,n$,
%where
%\begin{itemize}
%\item $X_i \in \R^{p}$
%\item $\E\epsilon_i = 0$ and $\E \epsilon \epsilon^{\top} = I_n$ (w.l.o.g. $\sigma^2 = 1$)
%\item $\X$ is the design matrix, and rank($\X) = r$
%\end{itemize}
%
%\vsp
%We'll consider various properties that may be of interest:
%\begin{itemize}
%\item Estimating $\beta$
%\item Doing good predictions
%\item Select the correct model
%\end{itemize}
%%We'll consider two loss functions, with associated risk (that is, integrated loss)
%%\begin{align}
%%R_{func}(\hat\beta)
%%\end{align}
%\end{frame}
%
%\begin{frame}
%\frametitle{Estimating $\beta$}
%To get $L^2$ consistency, we need to show that 
%
%\script{writing for this section $\hbetar{\lambda} \equiv \hat\beta_\lambda$}
%\[
%R(\hat\beta_{\lambda}) = \E_{\data} ||\hat\beta_{\lambda} - \beta||_2^2
%\]
%goes to zero.
%
%\vsp
%Again, we can decompose this as ($\E_{\data} \equiv \E$)
%\begin{align}
%R(\hat\beta) 
%& = 
%\E ||\hat\beta - \E\hat\beta||_2^2 + ||\E\hat\beta - \beta||_2^2  \\
%& =
%\tr \V \hat\beta + \sum_{j=1}^p(\E\hat\beta_j - \beta_j)^2
%\end{align}
%
%\end{frame}
%
%\begin{frame}
%\frametitle{Estimating $\beta$}
%Continuing from the previous slide 
%
%\script{Remember, $\hat\beta_{\lambda} = V (D^2 + \lambda I)^{-1} D U^{\top} Y$}
%\begin{align}
%R(\hat\beta) 
%& = 
%\tr \V \hat\beta_{\lambda} + ||\E\hat\beta_{\lambda} - \beta||_2^2 \\
%& = 
%\tr (\X^{\top}\X + \lambda)^{-1}\X^{\top}\X(\X^{\top}\X + \lambda)^{-1}+ \\
%& \qquad  +||((\X^{\top}\X + \lambda I)^{-1}\X^{\top}\X - I)\beta||_2^2 \\
%& = \textrm{variance} + \textrm{bias}^2
%%& = \tr D^2(D^2 + \lambda I)^{-2} +\\
%%& \qquad  +||((\X^{\top}\X + \lambda I)^{-1}\X^{\top}\X - I)\beta||_2^2 
%\end{align}
%
%Let's address each of these terms separately
%\end{frame}
%
%\begin{frame}
%\frametitle{Estimating $\beta$: Bias}
%For the bias, let's use the Woodbury matrix inversion lemma
%\[
%(A - BC^{-1}E)^{-1} = A^{-1} + A^{-1} B(C - EA^{-1}B)^{-1} E A^{-1}
%\]
%\script{See Henderson, Searle (1980), equation (12) for a statement and discussion}
%\begin{align}
%\textrm{bias}^2 
%& = ||((\underbrace{\X^{\top}\X}_{E} + \underbrace{\lambda I}_{C})^{-1}\X^{\top}\X \underbrace{- I}_{A^{-1}})\beta||_2^2 \\
%& = ||(I + (\X^{\top}\X)\lambda^{-1})^{-1}\beta||_2^2 \\
%& = \lambda^2||(\lambda I + \X^{\top}\X)^{-1}\beta||_2^2 \\
%& = \lambda^2||(\lambda I + V D^2 V^{\top})^{-1}\beta||_2^2 \\
%& \stackrel{n > p}{=} \lambda^2||(\lambda I + V D^2 V^{\top})^{-1}\beta||_2^2
%\end{align}
%\end{frame}
%
%\begin{frame}
%\frametitle{Estimating $\beta$: Variance}
%Likewise,
%\begin{align}
%\textrm{variance} 
%& = \tr (\X^{\top}\X + \lambda)^{-1}\X^{\top}\X(\X^{\top}\X + \lambda)^{-1} \\
%& = \tr D^2(D^2 + \lambda I)^{-2} 
%\end{align}
%
%\end{frame}
%
%\begin{frame}
%\frametitle{A tale of three lassos}
%You can choose $\lambda$ for lasso via AIC or BIC as well.
%
%\vsp
%\emphasis{8cm}{Reminder:}{The BIC has a stronger penalty on model complexity than AIC}
%
%\vsp
%Let's define the following goals:
%\begin{table}
%\begin{tabular}{lp{5.5cm}}
% \smallCapGreen{All important variables}   & Will keep all the important variables, but may
% keep many more \\
%\smallCapGreen{Only important variables}  & Will keep only important variables, but may discard
%some important variables \\
%\smallCapGreen{Prediction Accuracy} & Will keep those variables that help most with prediction
%\end{tabular}
%\end{table}
%\end{frame}  
%
%\begin{frame}
%\frametitle{A tale of three lassos}
%Depending on your goal, the tuning parameter can be chosen as follows:
%
%\vsp
%\begin{table}
%\begin{tabular}{lccc}
%        & CV & AIC&
%         BIC\\
%                                          \\
% \smallCapGreen{All important variables}   & &  \checkmark & \\
%\\
%\smallCapGreen{Only important variables}  & & &  \checkmark \\
%\\
%\smallCapGreen{Prediction Accuracy} & \checkmark & & 
%\\
%\end{tabular}
%\end{table}
%\end{frame}

\end{document}
