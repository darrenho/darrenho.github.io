\documentclass[12pt]{beamer}
%\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{_defsAndPackages675notation}
\usepackage{_defsAndPackages675beamer}

%\DeclareMathSizes{12}{12}{5}{12}
\newcommand{\parenthetical}[2]{#1  \scriptstyle \alr{( #2)}}
\begin{document}

\title{\alg{Support vector machines}}
\subtitle{\classTitle}
%\author{\alg{Darren Homrighausen, PhD}}
%\institute{\classTitle}
\date{}



\begin{frame}
\maketitle
%\titlepage
%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=1in]{.../figures/CSU_logo2.eps}
%\end{figure}
%
\organization
%
\end{frame}

\begin{frame}
\frametitle{Optimal separating hyperplanes}
A main initiative in early computer science was to find \alg{separating hyperplanes}
among groups of data

\script{Rosenblatt (1958) with the \alo{perceptron} algorithm}
\vsp

The issue is that if there is a separating hyperplane, there is an infinite number

\vsp
An \alg{optimal separating hyperplane} can be generated by finding \alo{support points} and
bisecting them.

\script{Sometimes \alg{optimal separating hyperplanes} are called \alg{maximum margin classifiers}}
\end{frame}

\begin{frame}
\frametitle{Basic linear geometry}
A hyperplane in $\R^p$ is  given by 
\[
\mathcal{H} = \{X \in \R^p :  h(X) = \beta_0 + \beta^{\top}X = 0\}
\]
\script{Usually it is assumed that $\norm{\beta}_2 = 1$}

\vsp
\begin{enumerate}
\item The vector $\beta$ is \alg{normal} to $\mathcal{H}$

\script{To see this, let $X,X' \in \mathcal{H}$.  Then $\beta^{\top}(X - X') = 0$}

\item \smallCapGreen{Important:} For any point $X \in \R^p$, the (signed) length of its orthogonal complement
to $\mathcal{H}$ is $h(X)$

\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Support vector machines (SVM)}
Let $Y_i \in \{-1,1\}$

%\script{w.l.o.g let $||\beta||_2 = 1$}
\script{It is common with SVMs to code $Y$ this way.  With logistic regression,
$Y$ is commonly phrased as $\{0,1\}$ due to the connection with Bernoulli trials}

\vsp
We will generalize this to supervisors with more than 2 levels at the end

\vsp
A classification rule induced by a hyperplane is
\[
g(X) = \textrm{sgn}(X^{\top}\beta + \beta_0)
\]
\end{frame}


\begin{frame}
\frametitle{Separating hyperplanes}
Our classification rule is based on a hyperplane $\mathcal{H}$
\[
g(X) = \textrm{sgn}(X^{\top}\beta + \beta_0)
\]


A \alo{correct} classification is one such that 
$h(X)Y > 0$ and $g(X)Y > 0$

\script{Why?}
\vsp

The larger the quantity $Yh(X)$, the more ``sure'' the classification

\script{\smallCapGreen{Reminder:} The signed distance to $\mathcal{H}$ is $h(X)$}

\vsp
Under classical \alo{separability}, we can find a function such that $Y_i h(X_i) > 0$ 

\script{That is, makes perfect training classifications via $g$}
\vsp


\end{frame}

\begin{frame}
\frametitle{Optimal separating hyperplane}
This idea can be encoded in the following \alo{convex program}

\[
\max_{\beta_0,\beta} M \textrm{  subject to}
\]
\[
Y_ih(X_i) \geq M \textrm{ for each } i  \textrm{ and } \norm{\beta}_2 = 1
\]

\smallCapGreen{Intuition:} 
\begin{itemize}
\item We know that $Y_i h(X_i) > 0$ $\; \Rightarrow g(X_i) = Y_i$.  Hence,
larger $Y_i h(X_i)$ $\; \Rightarrow$ ``more'' correct classification
\item For ``more'' to have any meaning,
we need to \alo{normalize} $\beta$, thus the other constraint
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Optimal separating hyperplane}
Let's take the original program:
\[
\max_{\beta_0,\beta} M \textrm{ subject to}
\]
\[
\alb{Y_ih(X_i)} \geq M \textrm{ for each } i \textrm{ and } \norm{\beta}_2 = 1
\]
and \alg{rewrite} it as

\[
\min_{\beta_0,\beta} \frac{1}{2}\norm{\beta}_2^2 \textrm{ subject to}
\]
\[
\alb{Y_ih(X_i)} \geq 1 \textrm{ for each } i
\]
\script{Replace $\alb{Y_ih(X_i)} \geq M $ with $\frac{1}{\norm{\beta}_2}\alb{Y_ih(X_i)} \geq M$, which 
redefines $\beta_0$}

\vsp


This is still a \alo{convex} optimization program: quadratic criterion, linear inequality constraints
\end{frame}

\begin{frame}
\frametitle{Optimal separating hyperplane}
Again, we can convert this \alo{constrained} optimization problem into  the \alo{Lagrangian} (primal) form
\[
\min_{\beta_0,\beta}\frac{1}{2}\norm{\beta}_2^2 - \sum_{i=1}^n\alpha_i[Y_i(X_i^{\top}\beta + \beta_0) - 1]
\]
In contrast to the lasso problem, there are now $n$ Lagrangian parameters $\alpha_1,\ldots,\alpha_n$

\script{There are $n$ constraints, after all}

\vsp
Everything is nice and smooth, so we can take derivatives..
\end{frame}

\begin{frame}
\frametitle{Optimal separating hyperplane}
\[
\frac{1}{2}\norm{\beta}_2^2 - \sum_{i=1}^n\alpha_i[Y_i(X_i^{\top}\beta + \beta_0) - 1]
\]
Derivatives with respect to $\beta$ and $\beta_0$:

\begin{itemize}
\item $\beta = \sum_{i=1}^n \alpha_i Y_iX_i$
\item $0 = \sum_{i=1}^n \alpha_i Y_i$
\end{itemize}
Substituting into the \alo{Lagrangian}:
\[
\textrm{wolfe dual}  = 	\sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{k=1}^n\alpha_i\alpha_kY_iY_kX_i^{\top}X_k
\]
\script{this is all subject to $\alpha_i \geq 0$}

\vsp
We want to \alo{maximize} $\textrm{wolfe dual}$
\end{frame}


\begin{frame}
\frametitle{Optimal separating hyperplane}
A side condition, known as \alg{complementary slackness} states\footnote{See the Karush-Kuhn-Tucker (KKT)
conditions}:
\[
\alr{\alpha_i} [1 - \alb{Y_ih(X_i)}  ] = 0 \textrm{ for all } i
\]
\script{The product of \alr{Lagrangian parameters} and \alb{inequalty constraint} equals 0}
\vsp

This implies either:
\begin{itemize}
\item $\alpha_i = 0$, which happens if the constraint $Y_i h(X_i) > 1$

\script{That is, when the constraint is \alo{non binding}}

\item$\alpha_i > 0$, which happens if the constraint $Y_i h(X_i) =1 $

\script{That is, when the constraint is  \alo{binding}}
\end{itemize}

%\script{See the Karush-Khun-Tucker theorem for generalizing equality constraints to inequality constraints}

\end{frame}

\begin{frame}
\frametitle{Optimal separating hyperplane}
Taking this relationship
\[
\alpha_i [Y_ih(X_i) - 1] = 0
\]
we see that, for $i = 1,\ldots,n$,

\begin{itemize}
\item The points $(X_i,Y_i)$ such that $\alpha_i > 0$ are \alg{support vectors}
\item The points $(X_i,Y_i)$ such that $\alpha_i =0$ are \alo{irrelevant} for classification
\end{itemize}
\script{Why?}


\vvvsp

\vvsp
\smallCapGreen{End result:}  $\hat{g}(X) = \textrm{sgn}(X^{\top}\hat{\beta} + \hat{\beta_0})$


\end{frame}

\transitionSlide{Support vector classifier}

\begin{frame}
\frametitle{Support vector classifier}
Of course, we can't realistically assume that the data are linearly separated (even in a transformed space)

\vsp
In this case, the previous program has no \alo{feasible} solution

\vsp
We need to introduce \alg{slack} variables, $\xi$, that allow for overlap among the classes

\vsp
These slack variables allow for us to encode \alo{training missclassifications} into the optimization problem
\end{frame}


\begin{frame}
\frametitle{Support vector classifier}
\[
\max_{\beta_0,\beta,\xi_1,\ldots,\xi_n} M \; \textrm{ subject to}
\]
\[
Y_ih(X_i) \geq M  \underbrace{\alr{(1- \xi_i), \xi_i \geq 0, \sum \xi_i \leq t}}_{\alr{new}}, \textrm{ for each } i
\]
\vsp

Note that
\begin{itemize}
\item  $t$ is a  \alg{tuning parameter}.  The literature usually refers to $t$ as a \alg{budget}

\script{Think: lasso}

\item The separable case corresponds to  $t = 0$
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Support vector classifier}
We can rewrite the problem again:
\[
\min_{\beta_0,\beta,\xi} \frac{1}{2} \norm{\beta}_2^2 \; \textrm{ subject to}
\]
\[
Y_ih(X_i) \geq 1  \underbrace{\alr{- \xi_i, \xi_i \geq 0, \sum \xi_i \leq t}}_{\alr{new}}, \textrm{ for each } i
\]
\script{Convex optimization program: quadratic criterion, linear inequality constraints.}

\vsp
Converting $\sum \xi_i \leq t$ to the \alo{Lagrangian} (primal):
\[
\min_{\beta_0,\beta} \frac{1}{2}\norm{\beta}_2^2 + \alb{\lambda\sum \xi_i} \; \textrm{ subject to}
\]
\[
Y_ih(X_i) \geq 1  - \xi_i, \alb{\xi_i \geq 0} , \textrm{ for each } i
\]
\script{Think: lasso.  $\alb{\lambda\sum \xi_i} + \alb{\xi_i \geq 0} \Rightarrow \lambda \norm{\xi}_1$}
\end{frame}


\begin{frame}
\frametitle{SVMs: slack variables}

The \alo{slack variables} give us insight into the problem

\vsp
\begin{itemize}
\item If $\xi_i = 0$, then that observation is on \alo{correct} the side of the \alg{margin}
\item If $\xi_i =\in (0,1]$, then that observation is on the \alo{incorrect} side of the \alg{margin},
but still correctly classified
\item If $\xi_i > 1$, then that observation is \alo{incorrectly} classified
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Support vector classifier}
Continuing to convert constraints to \alo{Lagrangian}
\[
\min_{\beta_0,\beta,\xi} \frac{1}{2}\norm{\beta}_2^2 
+ 
\lambda\sum \xi_i 
\underbrace{\alr{- \sum_{i=1}^n\alpha_i[Y_i(X_i^{\top}\beta + \beta_0) - (1-\xi_i)] - \sum_{i=1}^n \gamma_i \xi_i}}_{\alr{\textrm{remaining constraints}}}
\]
Necessary conditions (taking derivatives)
\vsp

\begin{itemize}
\item $\beta = \sum_{i=1}^n \alpha_iY_iX_i$
\item $0 = \sum_{i=1}^n \alpha_iY_i$
\item $\alpha_i = \lambda - \gamma_i$
\end{itemize}

\script{As well as positivity constraints on Lagrangian parameters}
\end{frame}


\begin{frame}
\frametitle{Support vector classifier}
Substituting, we reaquire the \alo{Wolfe dual}
\vsp

This, combined with the \alo{KKT} conditions uniquely characterize the solution:


\[
\max_{\alpha \textrm{ subject to: KKT + Wolfe dual}} \sum_{i=1}^n \alpha_i - 
\frac{1}{2} \sum_{i=1}^n\sum_{i'=1}^n \alpha_i \alpha_{i'} Y_i Y_{i'} X_{i}^{\top} X_{i'}
\]

\script{See Chapter 12.2.1  in ``Elements of Statistical Learning''}

\vsp
Note: the necessary conditions $\beta = \sum_{i=1}^n \alpha_iY_iX_i$
imply estimators of the form
\begin{itemize}
\item  $\hat\beta = \sum_{i=1}^n \hat{\alpha}_iY_iX_i$
\item  $\hat\beta^{\top}X = \sum_{i=1}^n \hat{\alpha}_iY_iX_i^{\top}X$
\end{itemize}


\end{frame}


\begin{frame}
\frametitle{SVMs: tuning parameter}
We can think of $t$ as a \alg{budget} for the problem

\vsp
If $t = 0$, then there is \alo{no budget} and we won't tolerate any margin violations

\vsp
If $t > 0$, then no more than $\lfloor t \rfloor$ observations can be misclassified

\vsp
A larger $t$ then leads to larger \alo{margins}

\script{we allow more margin violations}
\end{frame}

\begin{frame}
\frametitle{SVMs: tuning parameter}
\smallCapGreen{Further intuition:}
\vsp

Like the optimal hyperplane, only observations that violate the margin
determine $\mathcal{H}$

\vsp
A large $t$ allows for many violations, hence many observations factor into the fit

\vsp
A small $t$ means only a few observations do

\vsp
Hence, $t$ calibrates a bias/variance trade-off, as expected

\vsp
In practice, $t$ gets selected via cross-validation
\end{frame}

\begin{frame}
\frametitle{SVMs: tuning parameter}
\begin{figure}
\includegraphics[width=3in]{../figures/svmISLtuningParameter}
\caption*{Figure 9.7 in ISL}
\end{figure}
\end{frame}
%\begin{frame}
%\frametitle{SVMs}
%The corresponding \alo{Lagrange} function to the constrained optimization problem is
%\begin{align*}
%\ell_{SVM}(\beta,\beta_0,\xi)
% & = 
% \norm{\beta}_2/2 + C\sum \xi_i - \\
% & \qquad - \sum_{i=1}^n\gamma_i[Y_if(X_i) - (1  - \xi_i)] - \sum_{i=1}^n \lambda_i \xi_i 
%\end{align*}
%
%Minimize with respect to $\beta_0,\beta,\xi_i$ via partial derivatives:
%\begin{align*}
%\beta & = \sum_{i=1}^n  \gamma_i y_ix_i \\
%0 & = \sum_{i=1}^n \gamma_iy_i \\
%\gamma_i &= C - \lambda_i \\
%\gamma_i,\lambda_i,\xi_i & \geq 0
%\end{align*}
%
%\end{frame}
%
%\begin{frame}
%\frametitle{SVMs}
%Outline of optimization steps
%\begin{enumerate}
%\item Formulate constrained form of problem
%\item Convert to (\alo{primal}) Langragian form
%\item Take all relevant (sub)-derivatives and set to zero
%\item Substitute these conditions back into \alo{primal} Langrangian $\longrightarrow$ \alo{dual} Lagrangian
%
%\script{This forms a lower bound on the solution to the constrained primal form of objective}
%\item Form Karush-Kuhn-Tucker (KKT) conditions for inequality constraints
%\item Examine the conditions for \alg{strong duality} which implies that the primal and dual forms have the same solution
%
%\script{The usual method is via Slater's condition, which says strong duality holds 
%if it is a convex program with non-empty constraint region}
%\end{enumerate}
%\end{frame}
%
%\begin{frame}
%\frametitle{SVMs}
%Once minimizers for $ \hat\gamma$ are found, we can plug-in and get
%\[
%\hat\beta = \sum_{i=1}^n \hat\gamma_i Y_i X_i
%\]
%Due to the KKT conditions\Note:
%\begin{align}
%\gamma_i[Y_i(X_i^{\top} \beta +\beta_0) - (1-\xi_i)] & = 0 \label{eq:kkt1}\\
%\lambda_i\xi_i & = 0 \\
%Y_i(X_i^{\top} \beta +\beta_0) - (1-\xi_i) & = 0 \label{eq:kkt2}
%\end{align}
%\vsp
%The $\hat\gamma_i$ are nonzero only for $i$ such that (\ref{eq:kkt2}) holds (by (\ref{eq:kkt1}))
%
%\vsp
%These observations are called the \alg{support vectors}
%
%\end{frame}

%
%\begin{frame}
%\frametitle{SVMs}
%By the previous conditions, either $\hat\xi_i = 0$ or $\hat\gamma_i = C$ 
%
%\vsp
%Using the condition: $\gamma_i[Y_i(X_i^{\top} \beta +\beta_0) - (1-\xi_i)]  = 0$
%
%\vsp
%For support vector $i$, if $\hat\xi_i = 0$:
%\[
%Y_i(X_i^{\top} \beta +\beta_0) - (1-\xi_i) = 0 \Leftrightarrow  \hat\beta_0 = 1/Y_i - X_i^{\top} \hat\beta
%\]
%
%
%\script{These estimates are usually averaged to get a final estimate of $\beta_0$}
%
%\vsp
%Now, the final classification is given by
%\[
%\hat{Y}(x) = \textrm{sgn}(\hat{f}(x)) =  \textrm{sgn}(x^{\top}\hat\beta + \hat\beta_0)
%\]
%
%\vsp
%The tuning parameter is given by the \alo{cost} $C$
%\end{frame}

\begin{frame}[fragile]
\frametitle{Support vector classifier in \alr{R}}
A common package to use is \alr{e1071}

\vsp
\begin{blockcode}
X = matrix(rnorm(20*2),ncol=2)
Y = c(rep(-1,10),rep(1,10))
X[Y == 1,] = X[Y == 1,] + 1

col = rep(0,length(Y))
col[Y == -1] = rainbow(2)[1]
col[Y == 1] = rainbow(2)[2]

pch = rep(0,length(Y))
pch[Y == -1] = 16
pch[Y == 1] = 17

plot(X,col=col,pch=pch)
\end{blockcode}
\end{frame}

\begin{frame}
\frametitle{Support vector classifier in \alr{R}}
\begin{figure}
\includegraphics[width=3in]{../figures/svmISLRcode1}
\end{figure}
\end{frame}

\begin{frame}[fragile]
\frametitle{Support vector classifier in \alr{R}}
\begin{blockcode}
library(e1071)
dat  =data.frame(X=X, Y=as.factor(Y))
svmfit=svm(Y~., data=dat, kernel="linear", cost=cost)
\end{blockcode}

\smallCapGreen{Important:} Their definition of cost is the \alo{Lagrangian} version, which we 
defined as $\lambda$

\vsp
Hence, a \alo{small cost} means a large $t$ and a \alo{wider} margin
\end{frame}

\begin{frame}
\frametitle{Support vector classifier in \alr{R}}
\begin{tabular}{ccc}
\includegraphics[width=1.5in]{../figures/svmISLRcode2} &
\includegraphics[width=1.5in]{../figures/svmISLRcode3} &
\includegraphics[width=1.5in]{../figures/svmISLRcode4} \\
cost $= .1$ &  cost $= 1$ & cost $= 10$
\end{tabular}
\end{frame}

\begin{frame}[fragile]
\frametitle{Support vector classifier in \alr{R}}
\begin{blockcode}
tune.out = tune(svm,Y~.,data=dat,kernel="linear",
     ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
best.model = tune.out$best.model
\end{blockcode}

\vsp
Note that \alr{best.model} is an \alr{svm} object:
\begin{figure}
\includegraphics[width=2in]{../figures/svmISLRcode5}
\end{figure}

\end{frame}

%
%
%
%\transitionSlide{Recasting as a penalized method}
%
%\begin{frame}
%\frametitle{Surrogate losses}
%It is tempting to minimize (analogous to linear regression)
%\[
%\frac{1}{n}\sum_{i=1}^n \mathbf{1}_{Y_i \neq \hat{Y}(x)}(X_i) + \lambda \rho(\beta)
%\]
%for some penalty term $\rho$
%
%\vsp
%However, this is \alo{nonconvex}
%
%\vsp
%\smallCapGreen{Idea:} We can use a \alg{surrogate} loss that mimics this function while still being
%convex
%
%\vsp
%It turns out we have already done that!
%\begin{itemize}
%\item  \smallCapGreen{Hinge:} $[ 1 - Y f(X)]_+$
%\item  \smallCapGreen{Logistic:} $\log(1 + e^{-Y f(X)})$
%\end{itemize}
%\end{frame}
%
%
%
%\begin{frame}
%\frametitle{Support vector classifier}
%Recall the \alo{Lagrangian} (primal):
%\[
%\min_{\beta_0,\beta} \frac{1}{2}\norm{\beta}_2^2 + \lambda\sum \xi_i \; \textrm{ subject to}
%\]
%\[
%Y_ih(X_i) \geq 1  - \xi_i, \xi_i \geq 0 , \textrm{ for each } i
%\]
%\vsp
%
%It can be shown that this is equivalent to with $\Lambda = 1/\lambda$
%\[
%\min_{\beta_0,\beta} \sum_{i=1}^n [1-Y_ih(X_i)]_+ + \frac{\Lambda}{2}\norm{\beta}_2^2
%\]
%\end{frame}
%
%
%\begin{frame}
%\frametitle{Surrogate losses}
%\begin{figure}
%\centering
%\includegraphics[width=4in]{../figures/surrogate.pdf}
%\end{figure}
%\end{frame}


\begin{frame}
\frametitle{Next time: Kernel methods}
\smallCapGreen{Intuition:}
Many methods have linear decision boundaries

\vsp
We know that sometimes this isn't sufficient to represent data

\vsp
\smallCapGreen{Example:} Sometimes we need to included a polynomial effect or a log transform
in multiple regression

\vsp
Sometimes, a \alo{linear} boundary, but in a different space makes all the difference..
\vfill
\end{frame}

\end{document}
\begin{frame}
\frametitle{Kernel SVM}
If we return to step \alb{4.} from the SVM outline:

\vsp
``Substitute these conditions back into \alo{primal} Langrangian 

$\longrightarrow$ \alo{dual} Lagrangian''

\vsp
We get that this dual Lagrangian is:
\[
\ell_D(\gamma) = \sum_i \gamma_i - \frac{1}{2}\sum_i \sum_{i'} \gamma_i \gamma_{i'} Y_i Y_{i'} \alr{X_i^{\top}X_{i'}}
\]
with side conditions: $\gamma_i \in [0,C]$ and $\gamma^{\top}Y = 0$

\vsp
The term $\alr{X_i^{\top}X_{i'}} = \langle X_i, X_{i'} \rangle$ is an \alo{inner product}

\vsp
SVMs therefore depend on the covariates via an inner product only

\vsp
This leaves them ripe for a \alo{kernel method}
\end{frame}

\transitionSlide{Kernel Methods}

\begin{frame}
\frametitle{Three related methods}
The following are seemingly disparate methods
\begin{itemize}
\item \smallCapGreen{Smoothness penalization:} Regularizing a loss functions with a penalty on smoothness 

\script{Example: Kernel SVM}
\item \smallCapGreen{Feature creation:} Imposing a \alo{feature mapping} $\Phi: \R^p \rightarrow \mathcal{A}$
thus creating new features e.g. via polynomials or interactions

\script{Example: Regression splines or polynomial regression}
\item \smallCapGreen{Gaussian processes:} Modeling the regression function as a Gaussian process
with a given mean and covariance

\script{Example: Gaussian process regression}
\end{itemize}
\vsp

It turns out these concepts are all the same and each forms a \alg{reproducing kernel Hilbert space} (RKHS)

\script{Many of these ideas are in Wahba (1990).  It was introduced to the ML community in Vapnik et al. (1996)
and summarized in a nice review paper in Hofmann et al. (2008)}
\end{frame}

\begin{frame}
\frametitle{Kernel methods}
Suppose $k:\mathcal{A}\times\mathcal{A} \rightarrow \R$ is a 
\alg{positive definite} kernel

\script{This means $\int\int k(x,y)f(x)f(y) dxdy > 0$}

\vsp
To be concrete, think of $\mathcal{A} = \R^p$

\script{However, any set of objects will do as long as an \alo{inner product} can be defined}

\vsp
Let's consider the space of functions generated by the completion of
\[
\mathcal{H}_k = \{k(\cdot,y): y \in \R^p\}
\]
\script{This loosely speaking all functions of the form $f(x) = \sum_{j=1}^J \alpha_j k(x,y_j)$}
\end{frame}

\begin{frame}
\frametitle{Kernel methods}
Write the eigenvalue expansion of $k$ as
\[
k(x,y) = \sum_{j=1}^\infty \theta_j \phi_j(x)\phi_j(y)
\]
with

\begin{itemize}
\item $\theta_j \geq 0 \parenthetical{\qquad}{\textrm{nonnegative definite}}$
\item $\norm{(\theta_j)_{j=1}^\infty}_2 < \infty$
\end{itemize}
\script{This is called \alg{Mercer's theorem}, and such a $k$ is called a \alg{Mercer} kernel}
\vsp

We can write any $f \in \mathcal{H}_k$ with two constraints
\begin{itemize}
\item $f(x) = \sum_{j=1}^\infty f_j \phi_j(x)$
\item $\langle f, f \rangle_{\mathcal{H}_k} = \norm{f}_{\mathcal{H}_k}^2 = \sum_{j=1}^\infty f_j^2/\theta_j < \infty$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kernel methods via regularization}
After specifying a kernel function\footnote{Or crucially and equivalently a set of eigenfunctions and
eigenvalues} $k$, we can define an estimator via
\[
\min_{f \in \mathcal{H}_k} \hat\P \ell_f + \lambda \norm{f}_{\mathcal{H}_k}^2
\]

\vsp
This is a (potentially) infinite dimensional optimization problem 

\script{\alo{hard}, especially with a computer}

\vsp
It can be shown that the solution has the form
\[
\hat{f}(x) = \sum_{i=1}^n \beta_i k(x,x_i)
\]
\script{This is known as the \alg{representer theorem}}
\end{frame}

\begin{frame}
\frametitle{Kernel methods via regularization}

\[
\hat{f}(x) = \sum_{i=1}^n \beta_i \alr{k(x,x_i)}
\]
The terms $\alr{k(x,x_i)}$ are the \alg{representers}, as
\[
\langle k(\cdot,x_i), f \rangle_{\mathcal{H}_k} = f(x_i)
\]
and $\mathcal{H}_k$ is called a \alg{reproducing kernel Hilbert space} (RKHS) as
\[
\langle k(\cdot,x_i),k(\cdot,x_{i'}) \rangle_{\mathcal{H}_k} = k(x_i,x_{i'})
\]
\end{frame}

\begin{frame}
\frametitle{Kernel methods via regularization}
Due to these properties, we can write the optimization problem as
\[
\min_{\beta} \hat\P \ell_{\mathbf{K}\beta} + \lambda \beta^{\top} \mathbf{K}\beta
\]
where $\mathbf{K} = [k(x_i,x_{i'})]$

\vsp
This provides a prescription for forming an incredibly rich suite of estimators:

\vsp
Choose a
\begin{itemize}
\item kernel $k$
\item loss function $\ell$
\end{itemize}
and then minimize
\end{frame}

\begin{frame}
\frametitle{Kernel methods via regularization: Example}
Suppose that $\ell_{\mathbf{K}\beta}(Z) = (Y - \mathbf{K}\beta)^2$

\vsp
Then:
\[
\hat{\beta} = \argmin_{\beta} \hat\P \ell_{\mathbf{K}\beta} + \lambda \beta^{\top} \mathbf{K}\beta = (\mathbf{K} + \lambda I)^{-1}Y
\]
and
\[
\hat{f} = \mathbf{K}\hat\beta = \mathbf{K}(\mathbf{K} + \lambda I)^{-1}Y = (\lambda\mathbf{K}^{-1} + I)^{-1}Y
\]
are the \alo{fitted values}

\script{This should be compared with the notes on ridge regression}
\end{frame}

\begin{frame}
\frametitle{Kernel methods via regularization: Example}
Alternatively, statisticians have been including polynomial terms/interactions for ages

\vsp
Form
\[
k_d(x,y) = (x^{\top}y + 1)^d 
\]

\vsp
$k_d$ has $M = {p + d \choose d}$ eigenfunctions

\vsp These \alo{span} the space of polynomials in $\R^{p}$ with degree $d$
\end{frame}
\begin{frame}
\frametitle{Kernel methods via regularization: Example}

\smallCapGreen{Example:} Let $d = p = 2 \Rightarrow M = 6$ and
\begin{align*}
k(x,y) & = 1 + 2x_1y_1 + 2x_2y_2 + x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2  \\
& = 
\sum_{u = 1}^M \Phi_u(x) \Phi_u(y) \\
& =
\langle \Phi(x) , \Phi(y) \rangle
\end{align*}
where\Note
\[
\Phi(y)^{\top}  = (1, \sqrt{2}y_1,\sqrt{2}y_2,y_1^2,y_2^2,\sqrt{2}y_1y_2)
\]
\script{See Vapnik (1996) for more on this example}
\end{frame}

\begin{frame}
\frametitle{Kernel methods: Summary}
From this example, we see that we could have generated this same RKHS via:
\begin{itemize}
\item Specifying the eigenfunctions (or another set of functions with the same span) and projecting
\item Defining the kernel $k$ explicitly and minimizing
\item Forming the \alo{feature map} $\Phi$ directly and implicitly defining $k(x,y) = \langle\Phi(x),\Phi(y)\rangle$
\end{itemize}
This last technique corresponds to \alg{kernelization}, where inner products in the original covariate
space are replaced with inner products of \alo{features}
\end{frame}
\transitionSlide{Kernel SVMs}

\begin{frame}
\frametitle{Kernel SVM: A reminder}
The dual Lagrangian is:
\[
\ell_D(\gamma) = \sum_i \gamma_i - \frac{1}{2}\sum_i \sum_{i'} \gamma_i \gamma_{i'} Y_i Y_{i'} \alr{X_i^{\top}X_{i'}}
\]
with side conditions: $\gamma_i \in [0,C]$ and $\gamma^{\top}Y = 0$

\vsp
Let's replace the term $\alr{X_i^{\top}X_{i'}} = \langle X_i, X_{i'} \rangle$ with
$\langle \Phi(X_i), \Phi(X_{i'}) \rangle$

\end{frame}

\begin{frame}
\frametitle{Kernel SVMs}
Typically, specifying $\Phi$ is unnecessary, 

\vsp
We need only define a \alo{kernel} that is symmetric, positive definite

\vsp
Some common choices for SVMs:
\begin{itemize}
\item \smallCapGreen{Polynomial:} $k(x,y) = (1 + x^{\top}y)^d$
\item \smallCapGreen{Radial basis:} $k(x,y) = e^{-\lambda \norm{x-y}_{\tau}^\tau}$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kernel SVMs}
\alo{Reminder:} the solution form for SVM is
\[
\beta = \sum_{i=1}^n \gamma_i Y_i X_i
\]
Kernelized, this is
\[
\beta = \sum_{i=1}^n \gamma_i Y_i \Phi(X_i)
\]

\vsp
Therefore, the induced hyperplane is:
\[
f(x) = \Phi(x)^{\top} \beta + \beta_0 = \sum_{i=1}^n \gamma_i Y_i \langle \Phi(x), \Phi(X_i) \rangle + \beta_0
\]

\vsp
The final classification is still
\[
\hat{Y}(x) = \textrm{sgn}(f(x)) 
\]

\end{frame}

\begin{frame}
\frametitle{SVMs via penalization}
It is important to note that SVMs can be derived from \alo{penalized loss} methods

\vsp
Writing $f(x) = \Phi(x)^{\top} \beta + \beta_0$, consider
\[
\min_{\beta,\beta_0} \sum_{i=1}^n [ 1 - Y_i f(X_i)]_+ + \lambda\norm{\beta}_2^2 /2
\]

This optimization problem produces the same solution as using $C = 1/\lambda$

\end{frame}

\begin{frame}
\frametitle{SVMs in practice}
\smallCapGreen{General functions:} The basic SVM functions are in the C++ library \alr{libsvm} 
\vsp

\smallCapGreen{R package:} The \alr{R} package \alr{e1071} calls \alr{libsvm} 
\vsp

\smallCapGreen{Path algorithm:} \alr{{\tt svmpath}}

\script{Hastie et al (2004)}

\vsp

For a discussion and comparison see Karatzoglou, Meyer (2006).
\end{frame}

%\frametitle{Sensitivity and Specificity}
%Now, we can compare doing classification by rounding the Linear Regression model
%to rounding the GLM.  
%
%\vsp
%We need two concepts:
%\vsp
%
%\emphasis{8cm}{Sensitivity:} {Classifying a person as a `default' given that they 
%defaulted (this is like correctly rejecting the null hypothesis, i.e. power)}
%\emphasis{8cm}{Specificity:}{Classifying a person as `no default' given that they did not default 
%(this is like \textbf{not} committing a type I error)}
%
%\begin{table}
%\begin{tabular}{l|p{1.8cm}p{1.8cm}p{1.8cm}}
%& Training Error & Training Sensitivity & Training Specificity \\
%\hline
%Linear Reg. & 0.033 & 0.000 & 1.000 \\
%GLM & 0.027 & 0.300 & 0.995
%\end{tabular}
%\end{table}
%\end{frame}
%
%
%\begin{frame}[fragile]
%\frametitle{More than two levels to the response}
%You can use logistic regression when your response has more than two levels.  There are two
%cases:
%
%\vsp
%
%\emphasis{7.3cm}{Unordered response:}{Called \alg{multinomial logistic regression}.  Essentially, you fit logistic regressions for each level versus  a reference level (examples: \alb{eye color} or \alb{political preference})}
%\emphasis{7.3cm}{Ordered Response:  } {These are \alg{common slopes} or 
%\alg{proportional odds model} (examples: \alb{how strongly do you agree with a statement} or \alb{number of malformed limbs in
%an experiment with mice})}
%\end{frame}
%
%
\end{document}
