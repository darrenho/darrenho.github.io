\documentclass[12pt]{beamer}
%\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{_defsAndPackages675notation}
\usepackage{_defsAndPackages675beamer}

%\DeclareMathSizes{12}{12}{5}{12}
\newcommand{\parenthetical}[2]{#1  \scriptstyle \alr{( #2)}}
\begin{document}

\title{\alg{Boosting 2: Classification}}
\subtitle{\classTitle}
%\author{\alg{Darren Homrighausen, PhD}}
%\institute{\classTitle}
\date{}



\begin{frame}
\maketitle

\organization
%
\end{frame}


\begin{frame}[fragile]
\frametitle{Additive models (for classification)}
As squared error loss isn't quite right for classification, \alg{additive logistic regression} is a popular approach

\vsp
Suppose $Y \in \{-1,1\}$
\[
\log \left(\frac{\P(Y= 1| X)}{\P(Y = -1 | X)} \right)= \sum_{j=1}^p h_j(x_j) = h(X)
\]

\vsp
This gets inverted in the usual way to acquire a probability estimate
\[
\pi(X) = \P(Y = 1|X) = \frac{e^{h(X)}}{1 + e^{h(X)}}
\]
\script{$h(X) = X^{\top} \beta$ gives us (linear) logistic regression, with classifier $g(X) = \textrm{sgn}(h(X))$}

\vsp
These models are usually fit by numerically maximizing the binomial likelihood, and hence
enjoy all the asymptotic optimality features of MLEs
\end{frame}

%\begin{frame}[fragile]
%\frametitle{Additive models (for classification)}
%In linear GLMs, the MLEs are found via \alg{Fisher scoring}
%\vsp
%
%At its core, we are finding updates $\E[ \eta(X) + (Y-\mu)\frac{d\eta}{d\mu} | X]$
%
%\vsp
%Given estimates $\hat{\eta} =\hat\beta^{\top}X$ and $\hat\mu$, we form \alo{working responses}
%\[
%Z = \hat\eta + (Y - \hat\mu)\frac{d\eta}{d\mu}
%\]
%and observational weights
%\[
%W^{-1} = \left( \frac{d\eta}{d\mu}\right)^2\V Y|_{\mu = \hat\mu}
%\]
%\script{For logistic regression, $g(\mu) = \eta$, where $g$ is the logistic function}
%
%\vsp
%Iteratively regress $Z$ on $X$ with weights $W$, form $\hat\mu$, form $Z$,...
%
%\vsp
%This is continued until the \alo{deviance} doesn't change much
%\[
%\textrm{dev}(Y,\hat\mu) = 2[\log(Y) - \log(\hat\mu)]
%\]
%\end{frame}
\begin{frame}[fragile]
\frametitle{Additive models (for classification)}
\smallCapGreen{Example:}
In \alr{R}, this can be fit with the package \alr{gam}

\vsp
In the \alr{gam} package there is a dataset \alr{kyphosis}

\vsp
This dataset examines a disorder of the spine

\vsp
Let's look at two possible covariates \alb{Age} and \alb{Number}

\script{\alb{Number} refers to the number of vertebrae that were involved in a surgery}
\end{frame}

\begin{frame}[fragile]
\frametitle{Additive models (for classification)}
\begin{blockcode}
library(gam)
data(kyphosis)

out = gam(Kyphosis~s(Age,3),family=binomial,data=kyphosis)
out.pred = predict(out)
plot(sort(kyphosis$Age),out.pred[order(kyphosis$Age)],
       type='l',xlab='Age',ylab='log odds')
\end{blockcode}
\begin{figure}
\centering
\includegraphics[width=1.7in]{../figures/classificationGAMmarginalFit.pdf}
\end{figure}
\end{frame}


\begin{frame}[fragile]
\frametitle{Additive models (for classification)}
\begin{blockcode}
out = gam(Kyphosis ~ s(Age,3) + s(Number,3),
        family = binomial, data=kyphosis)
out.pred = predict(out)
plot(sort(kyphosis$Age),out.pred[order(kyphosis$Age)],
     type='l',xlab='Age',ylab='log odds')
plot(sort(kyphosis$Number),out.pred[order(kyphosis$Number)],
     type='l',xlab='Number',ylab='log odds')
\end{blockcode}
\begin{figure}
\centering
\includegraphics[width=1.7in]{../figures/classificationGAMbivariateFit1.pdf}
\includegraphics[width=1.7in]{../figures/classificationGAMbivariateFit2.pdf}
\end{figure}
\end{frame}
%
%\begin{frame}[fragile]
%\frametitle{Additive models (for classification)}
%Using this approach, we can form a generalized version of back fitting called \alg{local scoring}
%
%\vsp
%It looks like
%\begin{enumerate}
%\item Start with guesses $f_1(x_1),\ldots,f_p(x_p)$, $F(x) = \sum_{j=1}^p f_j(x_j)$ and $p(x)$
%\item Form \alo{working responses}
%\[
%Z = F(x) + \frac{\mathbf{1}(Y = 1) - p(x)}{p(x)(1-p(x))}
%\]
%\item Apply back fitting to $Z$ with observational weights $p(x)(1-p(x))$ to produce $f_k(x_k)$
%
%\script{This is the data analogue to estimating $\E[ \eta(X) + (Y-\mu)\frac{d\eta}{d\mu} | X]$, constrained
%only to an additive model}
%
%\item Repeat until convergence
%\end{enumerate}
%\script{See Hastie, Tibshirani (1986) for details. Note that this insight produces the \alg{LogitBoost} algorithm.}
%\end{frame}

\transitionSlide{Adaboost}

\begin{frame}[fragile]
\frametitle{AdaBoost outline}
We give an overview of `AdaBoost.M1.' 

\script{Freund and Schapire (1997)}

\vsp
First, train the classifier as usual

\script{This is done by setting $w_i \equiv 1/n$}

\vsp
At each step $b$, the misclassified observations have their weights increased

\script{Implicitly, this lowers the weight on correctly classified observations}

\vsp
A new classifier is trained which emphasizes the previous mistakes
\end{frame}

\begin{frame}[fragile]
\frametitle{AdaBoost algorithm}
\begin{enumerate}
\item Initialize $w_i \equiv 1/n$
\item For $b = 1,\ldots,B$
\begin{enumerate}
\item Fit $g_b(X)$ on $\data$, weighted by $w_i$
\item Compute
\[
R_b = \frac{\sum_{i=1}^n w_i \mathbf{1}(Y_i \neq g_b(X_i))}{\sum_{i=1}^n w_i}
\]

\item Find $\beta_b = \log((1-R_b)/R_b)$
\item Set $w_i \leftarrow w_i\exp\{\beta_b \mathbf{1}(Y_i \neq g_b(X_i))\}$
\end{enumerate}
\item \smallCapGreen{Output:} $g(X) = \textrm{sgn}\left(\sum_{b=1}^B \beta_b g_b(X)\right)$
\end{enumerate}
\end{frame}

\transitionSlide{Some supporting simulations}


\begin{frame}[fragile]
\frametitle{AdaBoost: Simulation}
Let's use the classifier \alg{trees}, but with `depth 2-stumps'

\vsp
These are trees, but constrained to have no more than 4 terminal nodes

\begin{figure}
\includegraphics[width=2.5in]{../figures/treeBoostingDataEqual.pdf}
\end{figure}
\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost: Increasing $B$ (train)}
\begin{figure}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteTrainEqual2.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteTrainEqual5.pdf} \\
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteTrainEqual20.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteTrainEqual100.pdf}
\end{figure}
\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost: Increasing $B$ (test)}
\begin{figure}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteEqual2.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteEqual5.pdf} \\
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteEqual20.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteEqual100.pdf}
\end{figure}
\end{frame}

\begin{frame}[fragile]
\frametitle{AdaBoost: Train vs. Test}
\begin{figure}
\includegraphics[width=1.7in]{../figures/treeBoostingDataEqual.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingUnprunedEqual.pdf} \\
\includegraphics[width=1.7in]{../figures/treeBoostingStumpEqual.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteEqual5.pdf}
\end{figure}
\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost: Simulation}
Let's change the simulation so that the class probabilities aren't the same
\begin{figure}
\includegraphics[width=2.5in]{../figures/treeBoostingDataUnequal.pdf}
\end{figure}
\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost: Increasing $B$ (train)}
\begin{figure}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteTrainUnequal2.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteTrainUnequal5.pdf} \\
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteTrainUnequal20.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteTrainUnequal100.pdf}
\end{figure}
\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost: Increasing $B$ (test)}
\begin{figure}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteUnequal2.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteUnequal5.pdf} \\
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteUnequal20.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteUnequal100.pdf}
\end{figure}
\end{frame}

\begin{frame}[fragile]
\frametitle{AdaBoost: Train vs. Test}
\begin{figure}
\includegraphics[width=1.7in]{../figures/treeBoostingDataUnequal.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingUnprunedUnequal.pdf} \\
\includegraphics[width=1.7in]{../figures/treeBoostingStumpUnequal.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingDiscreteUnequal5.pdf}
\end{figure}
\end{frame}


%
%\begin{frame}[fragile]
%\frametitle{AdaBoost algorithm}
%\begin{enumerate}
%\item Initialize $w_i \equiv 1/n$
%\item For $b = 1,\ldots,B$
%\begin{enumerate}
%\item Fit $g_b(x)$ on $\data$, weighted by $w_i$
%\item Compute
%\[
%R_b = \frac{\sum_{i=1}^n w_i \mathbf{1}(Y_i \neq f_b(X_i))}{\sum_{i=1}^n w_i}
%\]
%
%\item Find $\beta_b = \log((1-R_b)/R_b)$
%\item Set $w_i \leftarrow w_i\exp\{\beta_b \mathbf{1}(Y_i \neq f_b(X_i))\}$
%\end{enumerate}
%\item \smallCapGreen{Output:} $f(x) = \textrm{sgn}\left(\sum_{b=1}^B \beta_m g_m(x)\right)$
%\end{enumerate}
%\end{frame}

\transitionSlide{Back to Algorithms}
\begin{frame}[fragile]
\frametitle{AdaBoost}
This algorithm became known as `discrete AdaBoost'

\script{This is due to the base classifier returning a discrete label}

\vsp
This was adapted to real-valued predictions in Real AdaBoost

\script{In particular, probability estimates}

\vsp
This terminology was introduced in Friedman's seminal paper on Functional Gradient Boosting (2001)
\end{frame}


\begin{frame}[fragile]
\frametitle{Real AdaBoost }
\begin{enumerate}
\item Initialize $w_i \equiv 1/n$
\item For $b = 1,\ldots,B$
\begin{enumerate}
\item Fit the classifier on $\data$, weighted by $w_i$ and produce $p_b(X) = \hat{P}_w(Y = 1 | X)$
\item Set $h_b(X) \leftarrow \textcolor<2>{redmain}{\frac{1}{2}\log( p_b(X)/(1-p_b(X)))}$
\item Set $w_i \leftarrow w_i\exp\{-Y_i h_b(X_i)\}$
\end{enumerate}
\item \smallCapGreen{Output:} $g(X) = \textrm{sgn}\left(\sum_{b=1}^B h_b(X)\right)$
\end{enumerate}
This is referred to as \alg{Real AdaBoost} and it used the class probability estimates to construct the
contribution of the $b^{th}$ classifier, instead of the estimated label

\vsp
\script{The distinction between Discrete/Real AdaBoost is reminiscent of 1 vs. 1 and 1 vs. All multiclass classification}
\end{frame}

\begin{frame}[fragile]
\frametitle{Real AdaBoost: Increasing $B$ (test)}
\begin{figure}
\includegraphics[width=1.7in]{../figures/treeBoostingRealEqual5.pdf} 
\includegraphics[width=1.7in]{../figures/treeBoostingRealEqual20.pdf} \\
\includegraphics[width=1.7in]{../figures/treeBoostingRealEqual1000.pdf}
\includegraphics[width=1.7in]{../figures/treeBoostingRealEqual10000.pdf}
\end{figure}
\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost intuition}
\smallCapGreen{Question:} Why does this work?

\vsp
\smallCapGreen{One answer:} Boosting fits an additive model

\[
G_B(X) = \sum_{b=1}^B \beta_b \phi(X,\theta_b)
\]
where 
\begin{itemize}
\item $\beta$ are weights
\item $\phi$ is some \alo{base learner} that depends on parameters $\theta$

\script{\smallCapGreen{Example:} Trees with all of its splits and terminal node values}
\end{itemize}

\vsp
\smallCapGreen{Overall:} Both discrete and real AdaBoost can be interpreted as stage wise estimation procedures
for fitting additive logistic regression models
\end{frame}

\begin{frame}[fragile]
\frametitle{(Discrete) AdaBoost interpretation}

Forward stagewise  additive modeling:

\script{Using a general likelihood $\ell$}

\begin{enumerate}
\item 

$\beta_b,\theta_b = \argmin_{\beta,\theta}\sum_{i=1}^n\ell( Y_i, G_{b-1}(X_i) + \beta\phi(X_i,\theta))$

\item Set $G_b(X) = G_{b-1}(X) + \beta_b \phi(X; \theta_b)$
\end{enumerate}
\vsp
AdaBoost implicitly does this by use of the \alg{exponential} \alo{loss function}
\[
\ell(Y,G) = \exp\{-YG(X)\}
\]
and basis functions $\phi(x,\theta) = g_b(X)$
\end{frame}



\begin{frame}[fragile]
\frametitle{AdaBoost intuition}
Suppose we minimize exponential loss in a forward stagewise manner

\vsp
Doing the forward selection for this loss, we get

\begin{align*}
(\beta_b,g_b) 
& = \argmin_{\beta,g} \sum_{i=1}^n \exp\{-Y_i(G_{b-1}(X_i) + \beta g(X_i))\}\\
\end{align*}

\end{frame}




\begin{frame}[fragile]
\frametitle{AdaBoost intuition}
Rewriting:
\begin{align*}
(\beta_b,g_b) 
& = \argmin_{\beta,g} \sum_{i=1}^n \exp\{-Y_i(G_{b-1}(X_i) + \beta g(X_i))\}\\
& = \argmin_{\beta,g} \sum_{i=1}^n  \alr{\exp\{-Y_iG_{b-1}(X_i)\}}\alb{\exp\{-Y_i\beta g(X_i))\} }\\
& = \argmin_{\beta,g} \sum_{i=1}^n \alr{w_i} \alb{\exp\{-Y_i\beta g(X_i)\}}
\end{align*}
Where
\begin{itemize}
\item Define $\alr{w_i = \exp\{-Y_iG_{b-1}(X_i)\}}$ 

\script{This is independent of $\beta,g$}
\item  $\sum_{i=1}^n\alr{w_i}\alb{\exp\{ -Y_i\beta g_b(X_i))\}}$ needs to be optimized
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{AdaBoost intuition}
Note that
\begin{align*}
\sum_{i=1}^n\alr{w_i}\alb{\exp\{ -\beta Y_ig(X_i))\}}
& =
e^{-\beta}\sum_{i:Y_i = g(X_i)} w_i + e^{\beta}\sum_{i:Y_i \neq g(X_i)} w_i \\
& =
(e^{\beta} - e^{-\beta}) \sum_{i=1}^n w_i \mathbf{1}(Y_i \neq g(X_i))+ \\
&\qquad + e^{-\beta} \sum_{i=1}^n w_i
\end{align*}

\vsp
As long as $(e^{\beta} - e^{-\beta}) \geq 0$, we can find
\[
g_b = \argmin_g \sum_{i=1}^n w_i \mathbf{1}(Y_i \neq g(X_i))
\]
\script{Note: If $(e^{\beta} - e^{-\beta}) < 0$, then $\beta < 0$.  However, as $\beta_b = \log((1-R_b)/R_b)$,
this implies $R > 1/2$.  Hence, we would flip the labels and get $R \leq 1/2$.}
\end{frame}



\begin{frame}[fragile]
\frametitle{Reminder: AdaBoost}
\begin{enumerate}
\item Initialize $w_i \equiv 1/n$
\item For $b = 1,\ldots,B$
\begin{enumerate}
\item \textcolor<1>{redmain}{Fit $g_b(x)$ on $\data$, weighted by $w_i$}

\script{This step is finding the next best version of the classifier, trained on weighted data and
added to the previous classifiers}
\item Compute
\[
R_b = \frac{\sum_{i=1}^n w_i \mathbf{1}(Y_i \neq g_b(X_i))}{\sum_{i=1}^n w_i}
\]

\item \textcolor<2>{redmain}{Find $\beta_b = \log((1-R_b)/R_b)$}
\item Set $w_i \leftarrow w_i\exp\{\beta_b \mathbf{1}(Y_i \neq g_b(X_i))\}$
\end{enumerate}
\item \smallCapGreen{Output:} $g(X) = \textrm{sgn}\left(\sum_{b=1}^B \beta_b g_b(X)\right)$
\end{enumerate}
\end{frame}

\begin{frame}[fragile]
\frametitle{AdaBoost intuition}
\smallCapGreen{Goal:} Minimize
\[
\sum_{i=1}^nw_i\exp\{ -\beta Y_ig_b(X_i))\}
\]

\script{Here, we have fixed $g = g_b$}
\vsp

We showed this can be written
\begin{align*}
\sum_{i=1}^nw_i\exp\{ -\beta Y_ig_b(X_i))\}
& = 
(e^{\beta} - e^{-\beta}) R_bW+  e^{-\beta} W \parenthetical{\quad}{W = \sum w_i}
\end{align*}
Take derivative with respect to $\beta$
\[
(e^{\beta} + e^{-\beta}) R_bW-  e^{-\beta} W \stackrel{set}{=} 0 \stackrel{set}{=} e^{\beta} R_b + e^{-\beta}(R_b - 1)
\]
Solve for $\beta$ to find $\beta_b = 1/2\log[(1- R_b)/R_b]$
\end{frame}

\begin{frame}[fragile]
\frametitle{Reminder: AdaBoost}
\begin{enumerate}
\item Initialize $w_i \equiv 1/n$
\item For $b = 1,\ldots,B$
\begin{enumerate}
\item Fit $g_b(x)$ on $\data$, weighted by $w_i$

\script{This step is finding the next best version of the classifier, trained on weighted data and
added to the previous classifiers}
\item Compute
\[
R_b = \frac{\sum_{i=1}^n w_i \mathbf{1}(Y_i \neq g_b(X_i))}{\sum_{i=1}^n w_i}
\]

\item \textcolor<1>{redmain}{Find $\beta_b = \log((1-R_b)/R_b)$}
\item \textcolor<2>{redmain}{Set $w_i \leftarrow w_i\exp\{\beta_b \mathbf{1}(Y_i \neq g_b(X_i))\}$}
\end{enumerate}
\item \smallCapGreen{Output:} $g(x) = \textrm{sgn}\left(\sum_{b=1}^B \beta_b g_b(x)\right)$
\end{enumerate}
\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost intuition}
The approximation is updated
\[
G_b(X) = G_{b-1}(X) + \beta_bg_b(X)
\]
This causes the weights
\[
w_i^{(b+1)} = \exp\{-Y_iG_{b}(X_i)\} = w_i^{(b)} \exp\{-\beta_b Y_i g_b(X_i)\}
\]
Using $-Y_ig_b(X_i)  = 2 \mathbf{1}(Y_i \neq g_b(X_i))  - 1$, this becomes
\[
w_i^{(b+1)} \propto w_i^{(b)} \exp\{\beta_b \mathbf{1}(Y_i \neq g_b(X_i))\}
\]
where $\beta_b \leftarrow 2\beta_b$, giving the last step of the algorithm
\end{frame}


\begin{frame}[fragile]
\frametitle{Other loss functions}
\begin{figure}
\centering
\includegraphics[width=3in]{../figures/adaboostLoss}
\end{figure}
\script{Hastie et al (2009)}
\end{frame}



\begin{frame}[fragile]
\frametitle{AdaBoost: The controversy}
\smallCapGreen{Claim:}  Boosting is another version of bagging

\vsp
The early versions of Boosting involved (weighted) resampling

\vsp
Therefore, it was initially speculated that a connection with \alo{bagging} explained its performance

\vsp
However, boosting continues to work well when
\begin{itemize}
\item The algorithm is trained on weighted data rather than on sampling with weights

\script{This removes the randomization component that is essential to bagging}
\item Weak learners are used that have high bias and low variance

\script{This is the \alo{opposite}
of what is prescribed for bagging}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{AdaBoost: The controversy}
\smallCapGreen{Claim:}  Boosting fits an adaptive additive model which explains its effectiveness

\vsp
The previous results appeared in Friedman et al. (2000) and claimed to have `solved' the mystery of boosting

\vsp
A crucial property of boosting is that is essentially never over fits

\vsp
However, the additive model view really should translate into intuition of `over fitting is a major concern,' as it is with
additive models
\end{frame}

\begin{frame}[fragile]
\frametitle{AdaBoost: The controversy}
As adaBoost fits an additive model in the base classifier, it cannot have higher order interactions than the 
base classifier

\vsp
For instance, a stump would provide a purely additive fit

\script{It only splits on one variable.  In general, the complexity of a tree can be interpreted as the number of 
included interactions}

\vsp
It stands to reason, then, if the Bayes' rule is additive in a similar fashion, stumps should perform well in Boosting
\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost: The controversy}
A recent paper investigating this property did substantial simulations using underlying purely additive models

\script{Mease, Wyner (2008)}
\vsp

Here is an example figure from their paper:
\begin{figure}
\centering
\includegraphics[width=3in]{../figures/boostingStump}
\caption{Black, bold line: Stumps.  Red, thin line: 8-node trees}
\end{figure}

\end{frame}


\begin{frame}[fragile]
\frametitle{AdaBoost: The controversy continues}
Ultimately, interpretations are just modes of human comprehension

\vsp
The value of the insight is whether it provides fruitful thought about the idea

\vsp
From this perspective, AdaBoost fits an additive model.  

\vsp
However, many of the other connections are still of debatable value

\script{For example, LogitBoost}
\end{frame}


\begin{frame}[fragile]
\frametitle{Next lectures}
Discuss two current, popular algorithms and their \alr{R} implementations
\vsp

\begin{itemize}
\item \alr{GBM}
\item \alr{XGBoost}
\end{itemize}
\end{frame}

\end{document}
\transitionSlide{Additional topics}
\begin{frame}[fragile]
\frametitle{Tree complexity}
The \alg{base} classifier's complexity must be fixed

\vsp
As previously stated, it is usually chosen to be $M \in \{4,\ldots,8\}$

\vsp
This choice controls the number of \alo{interactions} available to the tree, and hence to boosting

\vfill

\end{frame}

\end{document}
