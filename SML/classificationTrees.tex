\documentclass[12pt]{beamer}
%\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{_defsAndPackages675notation}
\usepackage{_defsAndPackages675beamer}

%\DeclareMathSizes{12}{12}{5}{12}
\newcommand{\parenthetical}[2]{#1  \scriptstyle \alr{( #2)}}
\begin{document}

\title{\alg{Classification via Trees}}
\subtitle{\classTitle}
%\author{\alg{Darren Homrighausen, PhD}}
%\institute{\classTitle}
\date{}



\begin{frame}
\maketitle
%\titlepage
%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=1in]{....../../../figures/CSU_logo2.eps}
%\end{figure}
%
\organization
%
\end{frame}



\begin{frame}
\frametitle{What is a (decision) tree?}
\begin{itemize}
\item Trees involve \alo{stratifying} or \alo{segmenting} the predictor space into a number of simple regions.
\item Trees are simple and useful for interpretation.  
\item Basic trees are not great at prediction.
\item More modern methods that use trees are much better.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example tree}

\begin{figure}[h!]
  \centering
  \includegraphics[width=2.2in,trim=0 0 0 30,clip]
  {../figures/obamaTree.pdf}
\end{figure}
\end{frame}

%\begin{frame}[fragile]
%\frametitle{dendrogram view}
%
%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=2.3in,trim=0 50 0 50,clip]
%  {../figures/recessionTrees0a.pdf}
%\end{figure}
%\smallCapGreen{Terminology}
%\begin{itemize}
%\item We call each split or end point a \alo{node}.  Each terminal node is referred to as a \alo{leaf}.  
%\begin{itemize}
%\item[] This tree has 1 interior node and 2 terminal nodes. 
%\end{itemize}
%\item The interior nodes lead to \alo{branches}.  
%\begin{itemize}
%\item[] This graph has two main branches (the S\&P 500 split).
%\end{itemize}
%\end{itemize}
%\end{frame}
%


\begin{frame}[fragile]
\frametitle{Dendrogram view}

\begin{figure}[h!]
  \centering
  \includegraphics[width=2.3in,trim=0 50 0 50,clip]
  {../figures/recessionTrees0a.pdf}
\end{figure}
\smallCapGreen{Terminology}
\begin{itemize}
\item We call each split or end point a \alg{node}.  Each terminal node is referred to as a \alg{leaf}
\begin{itemize}
\item This tree has 2 interior nodes and 3 terminal nodes. 
\end{itemize}
\item The interior nodes lead to \alg{branches}.  
\begin{itemize}
\item This graph has two main branches (the S\&P 500 split).
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Partitioning view}
\begin{figure}[h!]
  \centering
  \includegraphics[width=2.2in]%,trim=50 100 50 20,clip]
  {../figures/recessionTrees0b.pdf}
\end{figure}
\smallCapGreen{Notes}
\begin{itemize}
\item We classify all observations in a region the same.
\item The three regions R1, R2, and R3 are the leaves of the tree.
\end{itemize}

%The predictions are the means of each rectangles.  
\end{frame}

\begin{frame}[fragile]
\frametitle{Tree}
\begin{figure}[h!]
  \centering
  \includegraphics[width=2in,trim=50 0 0 0,clip]
  {../figures/recessionTrees0a.pdf}
  \includegraphics[width=2.2in,trim=0 0 0 50,clip]
  {../figures/recessionTrees0b.pdf}
\end{figure}
We can interpret this as 
\begin{itemize}
\item S\&P 500 is the most important variable.
\item If S\&P 500 is large enough, then we predict no recession.
\item If S\&P 500 is small enough, then we need to know the change
  in the employment level of Maine.
\end{itemize}
\end{frame}


%\begin{frame}[fragile]
%\frametitle{When do trees do well?}
%\begin{figure}
%\centering
%\includegraphics[width=2.1in]{../figures/classTreeRegressionVsTree}
%\end{figure}
%Top Row: A two-dimensional classification example in which the true decision boundary is linear. 
%A linear boundary will outperform a decision tree that performs splits parallel to the axes.
%
%Bottom Row: Here the true decision boundary is non-linear. A linear model is unable to capture the true decision boundary, whereas a decision tree is successful.
%\end{frame}




\begin{frame}[fragile]
\frametitle{How do we build a tree?}

\begin{enumerate}
\item Divide the predictor space into
$M$ non-overlapping regions $R_1, \ldots, R_M$ 

\script{this is done via greedy, recursive, binary splitting}
\item Every observation that falls into a given region $R_m$ is given the same prediction
\begin{itemize}
\item \smallCapGreen{Regression:} The average of the responses for a region
\item \smallCapGreen{Classification:} Determined by majority (or plurality) vote in that region
\end{itemize}
\end{enumerate}
\vsp

Important:
\begin{itemize}
\item[-] Trees can only make rectangular regions that are \alo{aligned} with the coordinate axis.
\item[-] The fit is \alo{greedy}, which means that after a split is made, all further decisions are conditional
on that split.
\item[-] The tree stops splitting when there are too few observations in a terminal node
\end{itemize}
\end{frame}


\transitionSlide{Regression trees}

\begin{frame}[fragile]
\frametitle{Implicit model}
For a given partition $R_1, \ldots, R_M$, the model for the response is
\[
f(X) = \sum_{m = 1}^M c_m \mathbf{1}_{R_m}(X)
\]

\vsp
We need to estimate both $(R_m)$ and $(c_m)$

\vsp
Generally, searching over all possible regions is infeasible

\script{This would involve sifting through all $M \leq n$ and all configurations for $R_m$}

So we use a 
\alo{greedy} approach instead

\end{frame}

\begin{frame}[fragile]
\frametitle{Regression trees}

Define the two half-planes
\[
r_1(j,s) = \{X | X^j \leq s\} \qquad \textrm{and} \qquad r_2(j,s) = \{X | X^j > s\}
\]
\vsp

For squared error loss, we solve
\[
\min_{j,s}\left[ 
\min_{c_1} \sum_{X_i \in r_1(j,s)} (Y_i - c_1)^2  + 
\min_{c_2} \sum_{X_i \in r_2(j,s)} (Y_i - c_2)^2 
\right]
\]


This generates, for $n_k = \sum_{i=1}^n \mathbf{1}_{r_k}(X_i)$, 
\[
\hat{c}_k = n_k^{-1}\sum_{i:X_i \in r_k} Y_i
\]

The next splits will be conditional on the minimizing $\hat{s}$
\end{frame}

\transitionSlide{Classification trees}


\begin{frame}[fragile]
\frametitle{Classification trees}

For a given partition $R_m$ and class $g$, define training proportions
\[
\hat{p}_{mg}(X) = \mathbf{1}_{R_m}(X) n_m^{-1} \sum_{i:X_i \in R_m} \mathbf{1}(Y_i = g)
\]

\vsp
Our classification is
\[
\hat{g}(X) = \argmax_{g}  \hat{p}_{mg}(X)
\]

\vsp
This presumes a given partition $(R_m)$.  This must be estimated

\vsp
For this, we need a \alo{loss function}
\end{frame}

\begin{frame}[fragile]
\frametitle{How do we measure quality of fit?}
Different measures of \alg{node impurity} (loss function in tree terminology)

\vsp
There are many possibilities:
\begin{table}
\begin{tabular}{ll}
\smallCapGreen{classification error rate:} & $E = 1 - \max_g (\hat p_{mg})$ \\
\smallCapGreen{Gini index:}                       & $G = \sum_g \hat p_{mg}(1-\hat p_{mg})$ \\
\smallCapGreen{cross-entropy:}                 & $D = -\sum_g \hat p_{mg}\log(\hat p_{mg})$
\end{tabular}
\end{table}
\script{Cross-entropy is also known as deviance}

\vsp
We build a classifier by \alg{growing} a tree that \alo{greedily} minimizes one of these criteria
\end{frame}

\begin{frame}[fragile]
\frametitle{How do we measure quality of fit?}
\smallCapGreen{Example:} Suppose $G = 2$. Then $\hat p = \hat p_{m1} = 1-\hat p_{m2}$

\vsp
The $m^{th}$ node is made by minimizing $E$, $G$, or $D$ over all 
\begin{itemize}
\item Features
\item split points of that feature
\end{itemize}


\begin{figure}
\centering
\includegraphics[width=1.6in]{../figures/classificationQualOfFitLegend}
\end{figure}
Generally, \smallCapGreen{Gini index} or \smallCapGreen{Cross-entropy} is preferred

\script{They penalize values of $\hat p$  far from 0 or 1 more severely}
\end{frame}



\begin{frame}[fragile]
\frametitle{How do we measure quality of fit?}
\smallCapGreen{Example:} Suppose $G = 2$ and we want to make the first split

\vsp
Then $\hat p_{11} = 1-\hat p_{12}$

\script{Define the `left' or `bottom' region as $R_1$}

\vsp
Let's look at some possible splits:
\begin{figure}
\centering
\includegraphics[width=2.1in,trim = 0 0 0 50,clip]{../figures/classificationSplitsX1legend} 
\includegraphics[width=2.1in,trim = 0 0 0 50,clip]{../figures/classificationSplitsX2legend}
\end{figure}
\end{frame}

\begin{frame}[fragile]
\frametitle{How do we measure quality of fit?}
\begin{figure}
\centering
\includegraphics[width=2.3in,trim = 0 15 0 50,clip]{../figures/classificationSplitsX1legend} 
\includegraphics[width=2.3in,trim = 0 15 0 50,clip]{../figures/classificationSplitsX2legend}
\end{figure}
Where would we split?
\pause
\vsp

For $E$ and $G$, at the solid, horizontal line  

\script{$\hat{p}_{11} = 1 \Rightarrow E = 0, G = 20/81$}
\end{frame}

\begin{frame}[fragile]
\frametitle{How do we measure quality of fit?}
\begin{figure}
\centering
\includegraphics[width=2.3in,trim = 0 15 0 50,clip]{../figures/classificationSplitsX1legend} 
\includegraphics[width=2.3in,trim = 0 15 0 50,clip]{../figures/classificationSplitsX2legend}
\end{figure}
Where would we split if we required $\geq 2$ observations in a node?
\pause

\vsp
\script{At the dashed, vertical line  for $E$. At either dashed or dotted vertical line for $G$}
\end{frame}


\begin{frame}[fragile]
\frametitle{There's a problem}
Following this procedure {\huge \alo{overfits!}}
\vsp



\begin{itemize}
\item The process described so far will fit overly complex trees, leading to poor predictive performance.
\item Overfit trees mean they have too many leaves.
\item To stretch the analogy further, trees with too many leaves must be \alg{pruned}.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Pruning the tree}

\begin{itemize}
\item Cross-validation can be used to directly prune the tree, but it is far too expensive (computationally) to use in practice 
(combinatorial complexity)
\item Instead, we use \alg{weakest link pruning}
\[
\sum_{m=1}^{|T|} \sum_{i \in R_m} \mathbf{1}(Y_i \neq \hat Y_{R_m}) + \lambda |T|
\]
%\item $G = \sum_k \hat p_{mk}(1-\hat p_{mk})$: Gini index.  
where $|T|$ is the number of terminal nodes.  

\vsp
Essentially, we are trading \alo{training fit} (first term) with \alo{model
complexity} (second term)

\script{compare to lasso}
\item Now, cross-validation can be used to pick $\lambda$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Results of trees on recession data}
%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=2.3in,trim=50 0 0 50,clip]
%  {../figures/recessionTrees1unpruned.pdf}
%  \includegraphics[width=2.3in,trim=50 0 0 50,clip]
%  {../figures/recessionTrees1pruned.pdf}
%\end{figure}
\begin{table}[h!]
  \centering
  \begin{tabular}{cc}
  \includegraphics[width=2.1in,trim=45 70 25 30,clip]
  {../figures/recessionTrees1unpruned.pdf} &
  \includegraphics[width=2.1in,trim=45 70 25 30,clip]
  {../figures/recessionTrees1pruned.pdf} \\
  Unpruned tree & Pruned Tree
  \end{tabular}
\end{table}
\pause
The pruned tree is a \alo{subset} of the unpruned tree (\alg{nested})\\

\vsp
There are splits that result in having the same prediction.  \smallCapGreen{Why?}
\end{frame}


\begin{frame}[fragile]
\frametitle{Splits with same prediction}
\begin{figure}
\centering
\includegraphics[width=2.3in,trim = 0 15 0 50,clip]{../figures/classificationSplitSamePreds}
\end{figure}
Suppose we split at vertical, dashed line.
Then $\hat{p}_{11} = 0.75$.  

\vsp
What happens if we were to now split $R_1$ at $X2 = 0.5$?
\end{frame}

%
%
%\begin{frame}
%\frametitle{Results of trees on recession data}
%The trees on the previous slide were grown on the \alo{training} data (from 1960 to 2000)
%
%\vsp
%
%Now, we use them to predict on the \alo{test} data (from 2000 to 2011)
%\end{frame}
%%
%\begin{frame}
%\frametitle{Results of trees on recession data}
%
%\end{frame}


\begin{frame}[fragile]
\frametitle{Trees in \alr{R}}
Create a basic, unpruned tree:
\begin{blockcode}
require(tree)
out.tree = tree(Y~.,data=X,split='gini')
plot(out.tree)
text(out.tree)
\end{blockcode}
\end{frame}


\begin{frame}[fragile]
\frametitle{Trees in \alr{R}}
Prune the tree via \alo{cross-validation}
\begin{blockcode}
out.tree.orig = tree(Y~.,data=X)
out.tree.cv   = cv.tree(out.tree.orig,FUN=prune.misclass)
> names(out.tree.cv)
[1] "size"   "dev"    "k"      "method"
\end{blockcode}
\end{frame}

\begin{frame}[fragile]
\frametitle{Trees in \alr{R}}
Prune the tree via \alo{cross-validation}
\begin{blockcode}
> out.tree.cv
$size
[1] 14 13 11  9  3  2  1

$dev
[1] 45 45 44 44 44 64 67

$k
[1] -Inf  0.0  2.0  2.5  3.0 15.0 20.0

$method
[1] "misclass"
\end{blockcode}
\smallCapGreen{Note:} \\
\alr{k} corresponds to $\alb{\lambda}$ in weakest-link pruning.  \\
\alr{dev} means missclassifications
in \alr{cv.tree}

\end{frame}
\begin{frame}[fragile]
\frametitle{Cross validation plots}
\begin{blockcode}
plot(out.tree.cv$size,out.tree.cv$dev,type="b")
plot(out.tree.cv$k,out.tree.cv$dev,type="b")
\end{blockcode}
\begin{figure}[h!]
  \centering
  \includegraphics[width=2.5in,trim=0 0 0 30,clip]
  {../figures/recessionTreesCVplots.pdf}
\end{figure}
\end{frame}


\begin{frame}[fragile]
\frametitle{Trees in \alr{R}}
Prune the tree via \alo{cross-validation}
\begin{blockcode}
best.size  = out.tree.cv$size[which.min(out.tree.cv$dev)]
> best.size
[1] 11
out.tree   = prune.misclass(out.tree.orig,best=best.size)
class.tree = predict(out.tree,X_0,type='class')
\end{blockcode}
\end{frame}



\begin{frame}[fragile]
\frametitle{An Introductory Example}
Use macroeconomic data to predict recessions
\vsp

\vsp
Use handful of national-level variables -- Federal Funds Rate, Term
Spread, Industrial Production, Payroll Employment, S\&P500

\vsp
Also include state-level Payroll Employment

\vsp
In this example, we code $Y = 1$ as a recession and $Y = 0$ as growth.

\vsp
We will use data from 1960 through 1999  as \alo{training data} 
\vsp

We will use data from 2000 through 2011  as \alo{testing data} 


\end{frame}

\begin{frame}
\frametitle{Results of trees on recession data}
%\alr{These are awfully small. Can you regenerate the figures with
%  pch=19, cex=3 or something similar?}
\begin{table}[h!]
  \centering
\begin{tabular}{cc}
  \includegraphics[width=2.1in,height=2.15in,trim=0 10 20 55,clip]
  {../figures/recessionTrees2postProb.pdf} & 
  \includegraphics[width=2.1in,height=2.15in,trim=0 10 20 55,clip]
  {../figures/recessionTrees2class.pdf} \\
  Posterior probability of prediction & Predictions
\end{tabular}
\end{table}
\end{frame}

\begin{frame}
\frametitle{Advantages and disadvantages of trees}
\begin{itemize}
\item[+] Trees are very easy to explain (much easier than even linear regression).
\item[+] Some people believe that decision trees mirror human decision.
\item[+]  Trees can easily be displayed graphically no matter the dimension of the data.
\item[+] Trees can easily handle qualitative predictors without the need to create dummy variables.
\item[$-$] Trees aren't very good at prediction.
\end{itemize}
To fix this last one, we can try to grow many trees and average their performance. 
\end{frame}

\end{document}

